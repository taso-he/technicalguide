[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TASO Interactive Technical Guides",
    "section": "",
    "text": "Welcome to the TASO Interactive Technical Guides, a collection of interactive resources designed to guide and enhance your evaluation, data visualisation and coding practices. This website contains:\n\n\n\n  Data Infrastructure Guide  \n\nA practical guide on using institutional data to identify equality gaps in students success and evaluate the effectiveness of interventions designed to reduce them. Aims to help higher education providers generate robust type 3 (causal) evidence.\n\n\n\n  Data Visualisation Style Guide  \n\nDeveloped to help TASO, our partners, and the wider sector to create impactful charts. Designed to provide clear, accessible guidelines that you can apply using the data visualisation tool of your choice (e.g., Excel, R or Tableau).\n\n\n\n  Coding Good Practice  \n\nGuidelines, tools and tips for ensuring that code is accessible and shareable. Designed for those looking to improve the tidiness, reproducibility and quality of their R scripts.",
    "crumbs": [
      "Home",
      "Introduction to the guides"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "TASO Interactive Technical Guides",
    "section": "",
    "text": "Welcome to the TASO Interactive Technical Guides, a collection of interactive resources designed to guide and enhance your evaluation, data visualisation and coding practices. This website contains:\n\n\n\n  Data Infrastructure Guide  \n\nA practical guide on using institutional data to identify equality gaps in students success and evaluate the effectiveness of interventions designed to reduce them. Aims to help higher education providers generate robust type 3 (causal) evidence.\n\n\n\n  Data Visualisation Style Guide  \n\nDeveloped to help TASO, our partners, and the wider sector to create impactful charts. Designed to provide clear, accessible guidelines that you can apply using the data visualisation tool of your choice (e.g., Excel, R or Tableau).\n\n\n\n  Coding Good Practice  \n\nGuidelines, tools and tips for ensuring that code is accessible and shareable. Designed for those looking to improve the tidiness, reproducibility and quality of their R scripts.",
    "crumbs": [
      "Home",
      "Introduction to the guides"
    ]
  },
  {
    "objectID": "index.html#why-publish-on-github",
    "href": "index.html#why-publish-on-github",
    "title": "TASO Interactive Technical Guides",
    "section": "Why publish on GitHub?",
    "text": "Why publish on GitHub?\nWe created these documents using Quarto, and published them through GitHub. This enables a more fluid and responsive approach to sharing good practices. It allows us to regularly update the content, ensuring that you always have access to the latest guidance.\nThe interactive nature of this website also facilitates a more engaging experience. You will see callout boxes that highlight key points or warnings:\n\n\n\n\n\n\nTip\n\n\n\nThe data visualisation guide and coding good practice are separate but complementary documents. We designed the data visualisation style guide with a wide audience in mind. Those who do code can view the technical details behind the charts.\n\n\nYou will also see when we are referring to code through text highlighted like this. You can see code that you can copy and try yourself in these fold-out boxes:\n\n\nShow the code\nlibrary(tidyverse)\n\ntaso_three_colour &lt;- c(\"#3b66bc\", \"#00a8da\", \"#07dbb3\")\n\ndf &lt;- data.frame(\n  Year = c(2019, 2020, 2021, 2022),\n  School_A = c(10, 12, 15, 13),\n  School_B = c(17, 15, 21, 24),\n  School_C = c(4, 2, 5, 6)\n)\n\n\nYou can see the latest changes and the code behind how we built this website using Quarto on our GitHub page.\n\n\n\n\n\n\nLicensing and acknowledgements\n\n\n\nThe TASO Interactive Technical Guides © 2024 by TASO are licensed under CC BY-NC 4.0. This means that others are free to share, copy, distribute, and transmit the work, as well as to adapt or build upon it, for non-commercial purposes, as long as appropriate credit is given, a link to the license is provided, and any changes made are indicated.\nThis website incorporates and adapts elements of the Quarto code from the “Best Practices for Data Visualisation” guide, authored by Andreas Krause, Nicola Rennie and Brian Tarran, and published by the Royal Statistical Society in July 2023. While the specific content from the guide has not been used, the code structure has been valuable in the development of specific functionalities on this site. Their guide is accessible at https://royal-statistical-society.github.io/datavisguide/ and is licensed under CC BY 4.0. We are thankful to the authors and the Royal Statistical Society for sharing their expertise and resources, which have greatly assisted in the technical aspects of this website’s construction.",
    "crumbs": [
      "Home",
      "Introduction to the guides"
    ]
  },
  {
    "objectID": "dig/plan.html",
    "href": "dig/plan.html",
    "title": "Plan: Research questions and outcome measures",
    "section": "",
    "text": "PlanEvaluatorsPractitionersSenior management\nAfter diagnosing your equality gaps, the next step is to plan your evaluation. The first part of planning is to decide what your research questions and outcome measures are.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Research questions and outcomes"
    ]
  },
  {
    "objectID": "dig/plan.html#sec-types-of-evaluation",
    "href": "dig/plan.html#sec-types-of-evaluation",
    "title": "Plan: Research questions and outcome measures",
    "section": "Types of evaluation",
    "text": "Types of evaluation\nThe OfS categorises evaluation into three types:\n\nNarrative\nEmpirical enquiry\nCausal\n\nThese types are explained in Table 1. While this guide is focused predominantly on studies generating causal (type 3) evidence using institutional data, we have provided a flow chart to help you determine what kind of evaluation is most suitable and links to resources to help you achieve it.\n\n\n\n\nTable 1: OfS types of evaluation\n\n\n\n\n\n\nType of evidence\nDescription\nEvidence\nClaims you can make\n\n\n\n\nType 1 - narrative\nThe impact evaluation provides a narrative or a coherent theory of change to motivate its selection of activities in the context of a coherent strategy.\nEvidence of impact elsewhere and/or in the research literature on access and participation activity effectiveness or from your existing evaluation results.\nWe have a coherent explanation of what we do and why our claims are research-based.\n\n\nType 2 - empirical enquiry\nThe impact evaluation collects data on impact and reports evidence that those receiving an intervention have better outcomes, though does not establish any direct causal effect.\nQuantitative and/or qualitative evidence of a pre/post intervention change or a difference compared to what might otherwise have happened.\nWe can demonstrate that our interventions are associated with beneficial results.\n\n\nType 3 - causal\nThe impact evaluation methodology provides evidence of a causal effect of an intervention.\nQuantitative and/or qualitative evidence of a pre/post treatment change on participants relative to an appropriate control or comparison group who did not take part in the intervention.\nWe believe our intervention causes improvement and can demonstrate the difference using a control or comparison group.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Research questions and outcomes"
    ]
  },
  {
    "objectID": "dig/plan.html#identifying-research-questions-for-causal-evaluation",
    "href": "dig/plan.html#identifying-research-questions-for-causal-evaluation",
    "title": "Plan: Research questions and outcome measures",
    "section": "Identifying research questions for causal evaluation",
    "text": "Identifying research questions for causal evaluation\nThese questions follow the general format:\n\nDid the intervention change a specific outcome among the target group?\n\nFor example:\n\nDid peer mentoring reduce stress among nursing students?\n\n\nDid the online Cognitive Behavioural Therapy programme reduce depression among students?\n\nTo help formulate your questions, you should also consider:\n\nWho will use the findings and how?\nWhat do stakeholders need to learn from the evaluation?\nWhat questions will you be able to answer and when?",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Research questions and outcomes"
    ]
  },
  {
    "objectID": "dig/plan.html#identifying-outcome-measures",
    "href": "dig/plan.html#identifying-outcome-measures",
    "title": "Plan: Research questions and outcome measures",
    "section": "Identifying outcome measures",
    "text": "Identifying outcome measures\nOnce you have established your research questions, you will need to consider which outcome measures best enable you to answer them and demonstrate success. The measures should link closely with the process, outcomes and impact you have recorded in your theory of change. A simple way to think about which measures to select is:\n\nI’ll know the outcome has been reached when I see this indicator.\n\n\nI’ll know that student engagement has improved when I see average attendance has reached 70%.\n\n\n\n\n\n\n\nOutcome measures resources\n\n\n\nFor more information on identifying suitable outcome measures, see:\n\nTASO MEF: Plan\nPost-entry MOAT",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Research questions and outcomes"
    ]
  },
  {
    "objectID": "dig/plan-methods-overview.html",
    "href": "dig/plan-methods-overview.html",
    "title": "Plan: Methodology overview",
    "section": "",
    "text": "PlanEvaluatorsPractitionersSenior management\nThe main purpose of this guide is to help you use institutional data to evaluate student success, and the bulk of this page will focus on unpacking quantitative causal methods. We include explanations, examples, diagrams and key considerations for each quantitative causal methodology.\nHowever, after following the flowchart, you may have found that generating quantitative causal evidence isn’t feasible in your situation. This could be, for example, because your sample size is too small, or the outcomes you’re interested in are not meaningfully measurable in a quantitative manner. If this applies, please see our section on impact evaluation with small cohorts.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Methodology overview"
    ]
  },
  {
    "objectID": "dig/plan-methods-overview.html#sec-causal-evaluations",
    "href": "dig/plan-methods-overview.html#sec-causal-evaluations",
    "title": "Plan: Methodology overview",
    "section": "Causal evaluations (type 3)",
    "text": "Causal evaluations (type 3)\nDeciding which methods are most suitable to produce causal evidence can feel difficult. The flowchart and the following sections will help you explore which experimental or quasi-experimental methods might work for your evaluation.\nBefore using any specific experimental or quasi-experimental method, it is also important to consult more detailed resources. Each method comes with its own set of assumptions, requirements and considerations that will significantly impact the validity and reliability of your research findings.\n\n\n\n\n\n\nCausal evaluation resources\n\n\n\n\n\n\nTASO evaluation guidance\nTASO: Evaluating complex interventions using randomised controlled trials\nCausal Inference: The Mixtape (Scott Cunningham)\nMostly Harmless Econometrics (Joshua D. Angrist & Jörn-Steffen Pischke)\n\n\n\nDesigning Randomised Trials in Health, Education and the Social Sciences (David Torgerson & Carole Torgerson)\nImpact Evaluation in Practice (World Bank)\nEconometrics with R (Christoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer)\nCausal Inference for the Brave and True (Matheus Facure Alves)\n\n\n\n\n\nAs with the flowchart, these sections adapt and build on insights from the Magenta Book, published under the Open Government Licence.\n\nQuasi-experimental designs (QED)\n\nDifference-in-difference\n\nThis method estimates the effect of an intervention by comparing the difference in outcomes between those who received the intervention and those who did not, before and after the intervention took place.\nThis method helps to isolate the effect of receiving the intervention from other changes over time that might affect the outcome, allowing you to control for unobserved factors.\nThis method relies on the assumption that both groups would have followed similar trends in outcomes over time if the intervention wasn’t introduced (parallel trends assumption).\n\n\n\n\n\n\n\nDifference-in-difference example\n\n\n\n\n\n\n\n\nFigure 1: Difference-in-difference diagram\n\n\n\n\n\n\n\n\nA mentoring programme is designed to improve module grades.\nIt is rolled out to students in one academic department, but not to students in another similar department.\nThe students in both departments have similar characteristics.\nPrior to the intervention the trends in attainment over time were similar.\n\n\n\n\n\n\nConsiderations\n\nDifficult if there is no clear timing of the intervention.\nFor the most robust evaluation, requires outcome data before, during and after the intervention.\nIdentifying a suitable comparator group can be difficult.\nParallel trends assumption can be hard to establish and may not hold true.\n\n\n\n\n\nRegression discontinuity design\n\nThis method estimates the effect of an intervention for which participant eligibility depends on whether an individual exceeds a threshold or cutoff (e.g. module grade).\nIndividuals on either side of the cutoff are assumed to be similar, except for whether they received the intervention or not.\nBy comparing individuals just below the cutoff (and did not receive intervention) to those just above (and did receive the intervention), any difference in outcomes can be attributed to the intervention, providing causal evidence.\n\n\n\n\n\n\n\nRegression discontinuity design example\n\n\n\n\n\n\n\n\nFigure 2: Regression discontinuity design diagram\n\n\n\n\n\n\n\n\nA bursary is given to disadvantaged students achieving a final grade of 70 or higher at the end of their first year, aiming to increase retention.\nRetention rates of disadvantaged students who would have been eligible, but scored 69 (and a little below), are compared with students who scored 70 (and a little above) and received the bursary.\nThe comparison groups are clearly and objectively defined by the cutoff.\n\n\n\n\n\n\nConsiderations\n\nConclusions may not apply to those further from the cutoff\nAccurate estimation requires a large amount of data close to the cutoff.\n\n\n\n\n\nInstrumental variables\n\nThis method can be used when there is an external factor (the instrument) that influences whether someone participates in the intervention, but is not directly linked to the outcome being studied.\nA good instrumental variable is exogenous; that is the instrumental variable must affect the likelihood of taking part in the intervention but be unrelated to the outcome, other than through the intervention itself.\nThis method deals with hidden biases (such as motivation) and can address unobserved confounding variables. By comparing the outcomes of those influenced by the instrument to join the intervention to those who are not, the effect of the intervention can be determined.\n\n\n\n\n\n\n\nInstrumental variables example\n\n\n\n\n\n\n\n\nFigure 3: Instrumental variables diagram\n\n\n\n\n\n\n\n\nAn information and guidance session aims to improve graduate outcomes (e.g. employment).\nThe distance a student would have to travel to the session location might be a good instrument that could be both relevant to participation in the session and unrelated (external or exogenous) to the session outcomes could be the distance to the session location:\nRelevant: The distance between a student’s residence and the session location could influence their chances of attending.\nUnrelated: The distance a student travels to the session is unlikely to directly affect graduate outcomes. It influences outcomes primarily through its effect on session attendance.\n\n\n\n\n\n\nConsiderations\n\nFinding a valid instrument is hard: many factors will have some association with outcomes.\nPotential bias if the instrumental variable influences the outcome by a means unrelated to the intervention (i.e., it is not perfectly exogenous).\n\n\n\n\n\nMatching methods\n\nThese methods can be used to create a comparison group by matching individuals from the population who do not receive an intervention with individuals who do using characteristics (such as grades, entry route, gender, ethnicity) that could influence both the likelihood of receiving the intervention and the outcomes of interest.\nThe comparison group and intervention group will be similar based on those key characteristics. This creates pairs or groups that are comparable, except for the intervention.\n\n\n\n\n\n\n\nMatching methods example\n\n\n\n\n\n\n\n\nFigure 4: Matching methods diagram\n\n\n\n\n\n\n\n\nA new online tutoring programme for first-year students struggling with quantitative skills is introduced.\nTo evaluate its impact on grades, students who receive the programme are matched with similar students who do not, based on their entry route to higher education, prior module marks, and VLE engagement, creating comparable groups.\nThe grades in these groups are then compared.\nThis approach is intuitive and easy to understand, and can be more powerful when paired with a difference-in-difference method.\n\n\n\n\n\n\nConsiderations\n\nMatching can only control for observed variables; any differences in unobserved variables can bias the results.\nHeavily relies on the availability and quality of data on relevant covariates.\n\n\n\n\n\nInterrupted time-series analysis\n\nThis method can be used when an intervention takes place in the whole population of interest and the outcome measure is known over a period of time before and after an intervention (the interruption).\nThe effects of an intervention are evaluated by obtaining outcome measures at several points before and after an intervention is introduced, allowing the change in level and trend of outcomes to be compared.\n\n\n\n\n\n\n\nInterrupted time series example\n\n\n\n\n\n\n\n\nFigure 5: Interrupted time-series diagram\n\n\n\n\n\n\n\n\nA HEP introduces an institution-wide change in mental health and wellbeing support, which includes support for all students at key transition points and ‘mental health champions’ on every course.\nStudent outcomes across the HEP - such as wellbeing, engagement and attainment - before and after the change was introduced are compared.\n\n\n\n\n\n\nConsiderations\n\nRequires extensive data before and after the intervention (whole-provider change) comes in.\nConfidence in the causal impact of the intervention can be higher if there are no other changes occurring around the time of the intervention which could also affect the outcome. Consequently, unexpected events that occur around the time of the intervention may influence outcomes and confound the results.\nIf any changes in outcome and/or its trend do not match the intervention’s timing, then it is hard to support a cause-and-effect relationship.\n\n\n\n\n\nSynthetic control methods\n\nSimilar to a difference-in-difference design but uses data prior to the intervention to construct a ‘synthetic clone’ of a group receiving a particular intervention through the weighted average of groups not receiving the intervention.\nDifferences between the performance after the intervention begins between the intervention group and its synthetic clone may be used as evidence that the intervention has had an effect. Most commonly applied to interventions applied at an area or group level.\nOne key strength of synthetic controls is that they can offer a relevant comparison when no other comparators exist.\n\n\n\n\n\n\n\nSynthetic control methods example\n\n\n\n\n\n\n\n\nFigure 6: Synthetic control diagram\n\n\n\n\n\n\n\n\nA module within an academic department is trialling changes in assessments to see if this supports wellbeing and attainment.\nA synthetic control is created using a weighted average of student assessments from other modules.\nThe outcomes of this synthetic control prior to the change in assessment broadly match those of students on the reformed module.\nThe intervention effect can be inferred from the difference in outcomes at the end of the trial period.\n\n\n\n\n\n\nConsiderations\n\nOnly viable when it can be demonstrated that there was a relationship between the behaviour of the intervention and comparator groups in the period before the intervention.\nDeveloping a synthetic control requires extensive historical data: suitable when large volumes of secondary data are already available.\nCan be used on small sample sizes.\n\n\n\n\n\n\nRandomised controlled trials (RCTs)\n\nIndividual RCT\n\nIn a two-arm RCT individuals are randomly assigned to one of two groups. These two groups are usually the control group - who receive either no intervention orbusiness as usual - and the intervention group (sometimes referred to as the treatment group) who do receive the intervention.\nRandom assignment helps ensure that the individuals in each group are similar in all ways except for the intervention they receive.\nAny difference in outcomes between the groups is the intervention’s real effect.\nRCTs (with implementation and process evaluation) are often considered the strongest way of testing the effectiveness of an intervention.\n\n\n\n\n\n\n\nIndividual RCT example\n\n\n\n\n\n\n\n\nFigure 7: Individual RCT diagram\n\n\n\n\n\n\n\n\nA learning analytics system flags students who have low levels of engagement.\nEach student who is flagged is randomly assigned to receive either an email, or an email plus a phone call.\n\n\n\n\n\n\nConsiderations\n\nRunning an RCT can be expensive and take a lot of time.\nSometimes it is not possible or ethical to withhold the intervention from those who might benefit from it, making RCTs impractical; though see Waitlist or Stepped-wedge RCTs.\nRCTs are generally more ethical where there is no clear consensus about the effect of the intervention.\nThe generalisability may be limited if the study setting is very controlled.\n\n\n\n\n\nCluster RCT\n\nIn a two-arm cluster RCT, groups (clusters) of individuals are randomly assigned to one of two groups.\nThese two groups are usually the control group - who receive either no intervention or business as usual - and the intervention group (sometimes referred to as the treatment group) who do receive the intervention.\nCluster RCTs are used when interventions are delivered to pre-existing groups (e.g. students on a module) and individual random allocation is not possible.\nDelivering interventions to entire groups can reduce the risk of control group participants being exposed to the intervention (spillover effects).\n\n\n\n\n\n\n\nCluster RCT example\n\n\n\n\n\n\n\n\nFigure 8: Cluster RCT diagram\n\n\n\n\n\n\n\n\nParticipants are randomly assigned to receive a change in assessment type by academic school, with Business School and School of Social Sciences receiving the intervention, while School of Engineering and Mathematics and School of Life Sciences do not receive the intervention.\n\n\n\n\n\n\nConsiderations\n\nIf the clusters themselves are not randomly selected, there is a potential for selection bias.\nCluster RCTs often require larger samples than individual-level RCTs to achieve similar levels of statistical confidence, making them more resource-intensive.\n\n\n\n\n\nWaitlist or stepped-wedge RCT\n\nThese RCT methods are suitable when it is not ethical or desirable to permanently withhold an intervention. All participants eventually receive the intervention, which is especially important for interventions which we have strong reason to believe will be beneficial.\nCan be easier to implement than traditional RCTs.\nWaitlist RCT:\n\nParticipants are randomly assigned to either receive the intervention immediately or be placed on a waiting list (control group) and receive the intervention after a delay.\n\nStepped-wedge RCT:\n\nAll participants or clusters eventually receive the intervention, but the start times are randomly assigned so the intervention is rolled out in steps or phases.\nA stepped-wedge RCT can be thought of like a waitlist RCT with more than one delay period.\n\n\n\n\n\n\n\n\nWaitlist RCT example\n\n\n\n\n\n\n\n\nFigure 9: Waitlist RCT diagram\n\n\n\n\n\n\n\n\nWellbeing intervention targeted at STEM students has only 100 places per term, but 200 eligible students.\n100 students are randomly assigned to receive the intervention in term 1, while the other 100 are assigned to a waitlist control group.\nThe outcomes for both groups are measured at the end of term 1.\nThen in term 2, the waitlist control group receives the intervention.\n\n\n\n\n\n\nConsiderations\n\nIf the intervention’s effectiveness changes over time, the design might not capture this accurately.\nAs everyone ultimately receives the intervention, long-term outcomes cannot be measured.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Methodology overview"
    ]
  },
  {
    "objectID": "dig/plan-methods-overview.html#sec-correlational-evaluations",
    "href": "dig/plan-methods-overview.html#sec-correlational-evaluations",
    "title": "Plan: Methodology overview",
    "section": "Correlational evaluations (type 2)",
    "text": "Correlational evaluations (type 2)\nType 2 evidence might tell us that students who take part in an activity have better outcomes than other students, for example, we might compare attainment for students who take part in an activity versus those who don’t.\nThis kind of evidence allows us to understand if there is an association/correlation between taking part in an activity and better outcomes. However, it cannot tell us whether the activity actually causes the improvement or if other factors are responsible.\nThe same techniques used in identifying the gaps in the first place - visually or with inferential statistics (t-tests, ANOVA, regression) - can be used to monitor their progression once an intervention is in place.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Methodology overview"
    ]
  },
  {
    "objectID": "dig/plan-methods-overview.html#sec-small-n-evaluation",
    "href": "dig/plan-methods-overview.html#sec-small-n-evaluation",
    "title": "Plan: Methodology overview",
    "section": "Impact evaluation with small cohorts (small n evaluation)",
    "text": "Impact evaluation with small cohorts (small n evaluation)\nImpact evaluation is important but can be challenging. TASO promotes the use of rigorous experimental and quasi-experimental methodologies as these are often the best way to determine causal inference. However, sometimes these types of impact evaluation raise challenges:\n\nThey are effective in describing a causal link between an intervention and an outcome, but less good at explaining the mechanisms that cause the impact or the conditions under which an impact will occur.\nThey require a reasonably large number of cases that can be divided into two or more groups. Cases may be individual students or groups that contain individuals, such as classrooms, schools or neighbourhoods.\nMost types of experiment – and some types of quasi-experiment – require evaluators to be able to change or influence (manipulate) the programme or intervention that is being evaluated. However, this can be difficult or even impossible, perhaps because a programme or intervention is already being delivered and the participants already confirmed or because there are concerns about evaluators influencing eligibility criteria.\nExperimental and quasi-experimental evaluation methodologies can sometimes struggle to account for the complexity of programmes implemented within multifaceted systems where the relationship between the programme/intervention and outcome is not straightforward.\n\nAn alternative group of impact evaluation methodologies, sometimes referred to as ‘small n’ impact methodologies, can address some of these challenges:\n\nThey only need a small number of cases or even a single case. The case is understood to be a complex entity in which multiple causes interact. Cases could be individual students or groups of people, such as a class or a school. This can be helpful when a programme or intervention is designed for a small cohort or is being piloted with a small cohort.\nThey can ‘unpick’ relationships between causal factors that act together to produce outcomes. In small n methodologies, multiple causes are recognised and the focus of the impact evaluation switches from simple attribution to understanding the contribution of an intervention to a particular outcome. This can be helpful when services are implemented within complex systems.\nThey can work with emergent interventions where experimentation and adaptation are ongoing. Generally, experiments and quasi-experiments require a programme or intervention to be fixed before an impact evaluation can be performed. Small n methodologies can, in some instances, be deployed in interventions that are still changing and developing.\nThey can sometimes be applied retrospectively. Most experiments and some quasi-experiments need to be implemented at the start of the programme or intervention. Some small n methodologies can be used retrospectively on programmes or interventions that have finished.\n\n\n\n\n\n\n\nResources for impact evaluation with small cohorts\n\n\n\nTASO has some guidance for impact evaluation with small cohorts",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Methodology overview"
    ]
  },
  {
    "objectID": "dig/plan-methods-overview.html#implementation-and-process-evaluation",
    "href": "dig/plan-methods-overview.html#implementation-and-process-evaluation",
    "title": "Plan: Methodology overview",
    "section": "Implementation and process evaluation",
    "text": "Implementation and process evaluation\nThis type of evaluation provides information about how best to revise and improve activities. It can be used to assess whether the initiative, analysis and underlying assumption(s) are being implemented as intended. It is often helpful for pilot projects and new services and schemes. Implementation and process evaluation can also be used to monitor the progress and delivery of ongoing initiatives. IPE can help you:\n\nevaluate whether an intervention or programme was implemented as intended\nidentify the elements of an intervention or programme that are necessary to produce the intended effects (outcomes)\nestablish whether the assumption(s) and mechanisms underpinning the intervention’s theory of change hold true.\n\n\n\n\n\n\n\nImplementation and process evaluation resources\n\n\n\nTASO has published several resources to support the sector in conducting rigorous IPE:\n\n\n\nIPE guidance - provides an introduction to IPE and step-by-step process for completing an IPE\nIPE framework - provides a summary of the TASO IPE framework and step-by-step approach to implementing the guidance\nCase study - an illustrative example of an IPE protocol for an access intervention\n\n\n\nIPE protocol template - to guide the sector in designing and developing their own IPEs\nIPE reporting template - to guide the sector in structuring and developing their own IPE reports",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Methodology overview"
    ]
  },
  {
    "objectID": "dig/index.html",
    "href": "dig/index.html",
    "title": "Data infrastructure guide",
    "section": "",
    "text": "To create a more inclusive higher education system, institutions in England are required to take active steps to address barriers faced by underrepresented groups.\nSince 2018 the Office for Students (OfS), the regulator for higher education in England, has required all registered providers in the approved (fee cap) category to submit an access and participation plan (APP), which details how providers will support access and participation for underrepresented groups in higher education.\nThe regulations governing APPs were updated in 2023 to require higher education providers (HEPs) to identify risks to equality of opportunity in access, participation and progression, and provide details of interventions designed to eliminate these risks. In addition, HEPs are expected to evaluate the impact of these interventions and publish their results.\nThe purpose of this guide is to help HEPs use their institutional data to identify equality gaps in student success and evaluate the effectiveness of interventions designed to reduce them.",
    "crumbs": [
      "Home",
      "Data infrastructure guide"
    ]
  },
  {
    "objectID": "dig/index.html#what-is-this-document",
    "href": "dig/index.html#what-is-this-document",
    "title": "Data infrastructure guide",
    "section": "What is this document?",
    "text": "What is this document?\nThis document is a practical guide on using institutional data to address several key areas. It aims to help enable HEPs to:\n\nidentify equality gaps in higher education\nidentify where those equality gaps may be particularly acute, to enable effective design of post-entry student success activities\nhelp generate robust Type 3 (causal) evidence of the success of student support interventions.\n\nAdditionally, this guide provides considerations for:\n\nplanning an evaluation of student success interventions\nobtaining ethical approval to carry out an evaluation and publish the results of an intervention",
    "crumbs": [
      "Home",
      "Data infrastructure guide"
    ]
  },
  {
    "objectID": "dig/index.html#who-is-this-guide-for",
    "href": "dig/index.html#who-is-this-guide-for",
    "title": "Data infrastructure guide",
    "section": "Who is this guide for?",
    "text": "Who is this guide for?\nDifferent sections are aimed at different audiences but overall this document will be useful for:\n\n\n\nevaluators\npractitioners\nthose responsible for writing their institution’s APPs\nplanning teams\nsenior executive/management with responsibility for supporting student success.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn each section, it is indicated who that part of the guide is aimed at, e.g.:\n\n\nEvaluatorsPractitioners\n\n\n\n\n\n\nThere are a large variety of HEPs in England, from small and specialist providers with tens of students, to the very largest with tens of thousands. Where possible we have tried to make this guide useful for a wide range of providers, though some sections will be more relevant to some types of providers than others.",
    "crumbs": [
      "Home",
      "Data infrastructure guide"
    ]
  },
  {
    "objectID": "dig/index.html#how-is-the-guide-organised",
    "href": "dig/index.html#how-is-the-guide-organised",
    "title": "Data infrastructure guide",
    "section": "How is the guide organised?",
    "text": "How is the guide organised?\nThe guide is organised around the four stages of TASO’s Monitoring and Evaluation Framework (MEF), pictured in Figure 1. In addition, there are sections on common barriers to and facilitators of evaluation of student success work.\n\n\n\nFigure 1: MEF diagram\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe MEF icon will be used to signify the applicable part of the MEF for each section.\n\n\nPlan",
    "crumbs": [
      "Home",
      "Data infrastructure guide"
    ]
  },
  {
    "objectID": "dig/creating_dig_pdf.html",
    "href": "dig/creating_dig_pdf.html",
    "title": "creating_dig_pdf",
    "section": "",
    "text": "To save as pdf: render –&gt; Print –&gt; Pages: 2 - (n-1) –&gt; Untick Headers (keep Backgrounbd graphics ticked) –&gt; Save as PDF"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#what-is-this-document",
    "href": "dig/creating_dig_pdf.html#what-is-this-document",
    "title": "creating_dig_pdf",
    "section": "What is this document?",
    "text": "What is this document?"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#who-is-it-for",
    "href": "dig/creating_dig_pdf.html#who-is-it-for",
    "title": "creating_dig_pdf",
    "section": "Who is it for?",
    "text": "Who is it for?"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#how-is-the-guide-organised",
    "href": "dig/creating_dig_pdf.html#how-is-the-guide-organised",
    "title": "creating_dig_pdf",
    "section": "How is the guide organised?",
    "text": "How is the guide organised?"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#identifying-gaps-in-the-student-experience",
    "href": "dig/creating_dig_pdf.html#identifying-gaps-in-the-student-experience",
    "title": "creating_dig_pdf",
    "section": "Identifying gaps in the student experience",
    "text": "Identifying gaps in the student experience\n\nDescriptive analysis\n\n\nt-tests\n\n\nANOVA\n\n\nRegression\n\nSimple model\n\n\nSimple model with interaction term\n\n\nFull model (multiple regression)"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#design-your-intervention",
    "href": "dig/creating_dig_pdf.html#design-your-intervention",
    "title": "creating_dig_pdf",
    "section": "Design your intervention",
    "text": "Design your intervention\n\nTheory of change\n\n\nPost-entry typology"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#sec-types-of-evaluation",
    "href": "dig/creating_dig_pdf.html#sec-types-of-evaluation",
    "title": "creating_dig_pdf",
    "section": "Types of evaluation",
    "text": "Types of evaluation"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#identifying-research-questions-for-causal-evaluation",
    "href": "dig/creating_dig_pdf.html#identifying-research-questions-for-causal-evaluation",
    "title": "creating_dig_pdf",
    "section": "Identifying research questions for causal evaluation",
    "text": "Identifying research questions for causal evaluation"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#identifying-outcome-measures",
    "href": "dig/creating_dig_pdf.html#identifying-outcome-measures",
    "title": "creating_dig_pdf",
    "section": "Identifying outcome measures",
    "text": "Identifying outcome measures"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#sec-choosing-an-evaluation-design",
    "href": "dig/creating_dig_pdf.html#sec-choosing-an-evaluation-design",
    "title": "creating_dig_pdf",
    "section": "Choosing an Evaluation Design",
    "text": "Choosing an Evaluation Design"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#planning-the-analysis",
    "href": "dig/creating_dig_pdf.html#planning-the-analysis",
    "title": "creating_dig_pdf",
    "section": "Planning the analysis",
    "text": "Planning the analysis\n\nResourcing checklist\n\nData\n\n\nPeople\n\n\nSystems\n\n\n\nWriting an analysis plan/trial protocol\n\n\nEthical approval"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#sec-causal-evaluations",
    "href": "dig/creating_dig_pdf.html#sec-causal-evaluations",
    "title": "creating_dig_pdf",
    "section": "Causal evaluations (type 3)",
    "text": "Causal evaluations (type 3)\n\nQuasi-experimental designs (QED)\n\nDifference in difference\n\nConsiderations\n\n\n\nRegression discontinuity design\n\nConsiderations\n\n\n\nInstrumental Variables\n\nConsiderations\n\n\n\nMatching methods\n\nConsiderations\n\n\n\nInterrupted time-series analysis\n\nConsiderations\n\n\n\nSynthetic control methods\n\nConsiderations\n\n\n\n\nRandomised controlled trials (RCTs)\n\nIndividual RCT\n\nConsiderations\n\n\n\nCluster RCT\n\nConsiderations\n\n\n\nMethod Waitlist or Stepped-wedge RCT\n\nConsiderations"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#sec-correlational-evaluations",
    "href": "dig/creating_dig_pdf.html#sec-correlational-evaluations",
    "title": "creating_dig_pdf",
    "section": "Correlational evaluations (type 2)",
    "text": "Correlational evaluations (type 2)"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#sec-small-n-evaluation",
    "href": "dig/creating_dig_pdf.html#sec-small-n-evaluation",
    "title": "creating_dig_pdf",
    "section": "Impact evaluation with small cohorts (small n evaluation)",
    "text": "Impact evaluation with small cohorts (small n evaluation)"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#implementation-and-process-evaluation",
    "href": "dig/creating_dig_pdf.html#implementation-and-process-evaluation",
    "title": "creating_dig_pdf",
    "section": "Implementation and process evaluation",
    "text": "Implementation and process evaluation"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#qeds",
    "href": "dig/creating_dig_pdf.html#qeds",
    "title": "creating_dig_pdf",
    "section": "QEDs",
    "text": "QEDs\n\nMatching\n\n\nDifferences in differences"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#rcts",
    "href": "dig/creating_dig_pdf.html#rcts",
    "title": "creating_dig_pdf",
    "section": "RCTs",
    "text": "RCTs"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#reporting",
    "href": "dig/creating_dig_pdf.html#reporting",
    "title": "creating_dig_pdf",
    "section": "Reporting",
    "text": "Reporting"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#putting-evidence-to-work",
    "href": "dig/creating_dig_pdf.html#putting-evidence-to-work",
    "title": "creating_dig_pdf",
    "section": "Putting Evidence to Work",
    "text": "Putting Evidence to Work"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#barriers-to-effective-evaluation",
    "href": "dig/creating_dig_pdf.html#barriers-to-effective-evaluation",
    "title": "creating_dig_pdf",
    "section": "Barriers to effective evaluation",
    "text": "Barriers to effective evaluation"
  },
  {
    "objectID": "dig/creating_dig_pdf.html#facilitators-of-effective-evaluation",
    "href": "dig/creating_dig_pdf.html#facilitators-of-effective-evaluation",
    "title": "creating_dig_pdf",
    "section": "Facilitators of effective evaluation",
    "text": "Facilitators of effective evaluation"
  },
  {
    "objectID": "dig/acknowledgements.html",
    "href": "dig/acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "This guide has been written by Luke Arundel and Rob Summers. The authors would like to thank the following individuals who worked on the Institutional Data Use project in various capacities. Their valuable insights, gained through workshops and discussions, have greatly informed the content of this guide.\n\nBec Aeddi (Nottingham Trent University)\nAnna Anthony (HEAT)\nSally Andrews (Staffordshire University)\nFabio Arico (University of East Anglia)\nJarek Bryk (University of Huddersfield)\nHetti Bywater (TASO)\nSarah Conner (TASO)\nPete Crowson (Nottingham Trent University)\nSarah-Jane Crowson (Hereford College of Arts)\nTatjana Damjanovic (TASO)\nBecky Ghani (TASO)\nHelena Gillespe (University of East Anglia)\nCarl Harrington (University of East Anglia)\nMichelle Hawthorne (University of East Anglia)\nLaura Hope (Nottingham Trent University)\nMatthew Horton (Wolverhampton University)\nEmma Hynd (Nottingham Trent University)\nMike Kerrigan (Nottingham Trent University)\nEliza Kozman (TASO)\nMatt Mills (Huddersfield University)\nGary McGladdery (Nottingham Trent University)\nReda Nausedaite (University of East Anglia)\nMatt Pawelski (Lancaster University)\nGuy Pattison (Nottingham Trent University)\nNicholette Pollard-Odle (TASO)\nRebecca Robinson (Lancaster University)\nRain Sherlock (TASO)\nRebecca Siddle (Nottingham Trent University)\nMyles Smith (University of East Anglia)\nSarah Ward (University of East Anglia)\nJessie Whaite (Lancaster University)\nJane Wormold (Huddersfield University)\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Acknowledgements"
    ]
  },
  {
    "objectID": "data-vis/references.html",
    "href": "data-vis/references.html",
    "title": "References and further resources",
    "section": "",
    "text": "Financial Times. 2019. Visual vocabulary. https://ft-interactive.github.io/visual-vocabulary/ [Accessed 15 August 2024]\nSchwabish, J. 2021. Better data visualizations: A guide for scholars, researchers, and wonks. Columbia University Press.\nDonnarumma, F. 2019. Say what you see - the way we write chart titles is changing. https://digitalblog.ons.gov.uk/2019/01/28/ [Accessed 15 August 2024]\nLearn UI Design. 2023. Data color picker. https://www.learnui.design/tools/data-color-picker.html [Accessed 15 August 2024]\nGovernment Analysis Function. 2022. Data visualisation: charts. https://analysisfunction.civilservice.gov.uk/policy-store/data-visualisation-charts/ [Accessed 15 August 2024]\nRoyal Statistical Society. 2023. Best Practices for Data Visualisation: https://royal-statistical-society.github.io/datavisguide/ [Accessed 21 November 2023]\nCesal, A. 2020. Writing Alt Text for Data Visualization. https://medium.com/nightingale/writing-alt-text-for-data-visualization-2a218ef43f81 [Accessed 21 November 2023]",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "References and further resources"
    ]
  },
  {
    "objectID": "data-vis/references.html#resources-referenced-and-linked-to-in-the-text-in-order-of-appearance",
    "href": "data-vis/references.html#resources-referenced-and-linked-to-in-the-text-in-order-of-appearance",
    "title": "References and further resources",
    "section": "",
    "text": "Financial Times. 2019. Visual vocabulary. https://ft-interactive.github.io/visual-vocabulary/ [Accessed 15 August 2024]\nSchwabish, J. 2021. Better data visualizations: A guide for scholars, researchers, and wonks. Columbia University Press.\nDonnarumma, F. 2019. Say what you see - the way we write chart titles is changing. https://digitalblog.ons.gov.uk/2019/01/28/ [Accessed 15 August 2024]\nLearn UI Design. 2023. Data color picker. https://www.learnui.design/tools/data-color-picker.html [Accessed 15 August 2024]\nGovernment Analysis Function. 2022. Data visualisation: charts. https://analysisfunction.civilservice.gov.uk/policy-store/data-visualisation-charts/ [Accessed 15 August 2024]\nRoyal Statistical Society. 2023. Best Practices for Data Visualisation: https://royal-statistical-society.github.io/datavisguide/ [Accessed 21 November 2023]\nCesal, A. 2020. Writing Alt Text for Data Visualization. https://medium.com/nightingale/writing-alt-text-for-data-visualization-2a218ef43f81 [Accessed 21 November 2023]",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "References and further resources"
    ]
  },
  {
    "objectID": "data-vis/references.html#further-data-visualisation-resources",
    "href": "data-vis/references.html#further-data-visualisation-resources",
    "title": "References and further resources",
    "section": "Further data visualisation resources",
    "text": "Further data visualisation resources\nThis style guide draws on some existing style guides. Some key sources of inspiration include:\n\nGovernment Analysis Function style guide\nUrban Institute Data Visualisation style guide\nRoyal Statistical Society style guide\n\nWe have endeavoured to keep this guide as concise as possible, meaning we do not cover the same ground that more extensive style guides do. If you have further questions on data visualisation, the above guides may be able to help. In particular, we found the Government Analysis Function valuable as an extensive but readable style guide.\nOur principles for effective data visualisation are rooted in the insights of Jonathan Schwabish, showcased in his contributions through PolicyViz. Sources include:\n\nBetter Data Vizualisations: A Guide for Scholars, Researchers and Wonks\nBetter Presentations: A Guide for Scholars, Researchers and Wonks\nData viz cheat sheet\nList of style guides\nAn Economist’s Guide to Visualising Data\n\nFor excellent examples of data visualisation, we also found the Financial Times (FT) and The Economist very helpful, particularly the work of John Burn-Murdoch:\n\nYou can find the work of John Burn-Murdoch here, or on X (formerly Twitter). This presentation is also insightful.\nThe Economist’s data visualisation newsletter Off The Charts is a helpful resource for learning how their data journalism team create visualisations and tackles problems. Its ‘Graphic Detail’ section showcases its charts.\n\nOrganisations that focus on data visualisation also have helpful blogs. These include Flourish, Datawrapper and Visualisingdata.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "References and further resources"
    ]
  },
  {
    "objectID": "data-vis/references.html#r-resources",
    "href": "data-vis/references.html#r-resources",
    "title": "References and further resources",
    "section": "R resources",
    "text": "R resources\nYou can find example R code for creating charts with TASO stylings on our GitHub.\nIf you are using R to create your plots, you may also find the following resources helpful.\n\nggplot Wizardry Hands-On - some helpful tips and tricks from Cédric Scherer\nTips and Tricks for ggplot2 - more helpful tips and tricks from Erik Gahner Larsen\nModern Data Visualisation with R - an open-access textbook by Robert Kabacoff\nR for Data Science - an open-access textbook by Hadley Wickham, if you are using ggplot for the first time the data visualisation chapter provides a great introduction to how ggplot works.\nR graph gallery - a collection of charts made with R. The creator Yan Holtz provides useful insight on X (formerly Twitter).",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "References and further resources"
    ]
  },
  {
    "objectID": "data-vis/index.html",
    "href": "data-vis/index.html",
    "title": "Data visualisation style guide",
    "section": "",
    "text": "This data visualisation style guide has been developed to help TASO, our partners, and the wider sector to create impactful and effective charts. It is designed to provide clear, accessible guidelines that can be applied using the data visualisation tool of your choice. We welcome your feedback on this guidance. We view this as an iterative document that will evolve as we learn more about which elements are most helpful. Please let us know what you think: research@taso.org.uk.",
    "crumbs": [
      "Home",
      "Data visualisation style guide"
    ]
  },
  {
    "objectID": "data-vis/index.html#why-good-data-visualisation-matters",
    "href": "data-vis/index.html#why-good-data-visualisation-matters",
    "title": "Data visualisation style guide",
    "section": "Why good data visualisation matters",
    "text": "Why good data visualisation matters\nWhile often overshadowed by other aspects of writing and analysis, data visualisation warrants dedicated attention. At TASO, we recognise its value and are committed to empowering our audience through carefully designed charts. Good data visualisation is vital because it holds the power to convey complex information succinctly and memorably. By presenting data visually, we simplify patterns, trends and insights. We can tell a story with the data, and design our charts to tell this story compellingly. To do this, we need to carefully consider how we set up our charts, and not rely on the default software settings.",
    "crumbs": [
      "Home",
      "Data visualisation style guide"
    ]
  },
  {
    "objectID": "data-vis/index.html#about-the-guide",
    "href": "data-vis/index.html#about-the-guide",
    "title": "Data visualisation style guide",
    "section": "About the guide",
    "text": "About the guide\n\nHow to use the guide\nThe guide is split into two main sections.\n\nThe first section is designed for anyone who wants to make their data visualisations more engaging. We present guidelines that serve as effective rules of thumb for creating impactful charts. While we expect our staff and partners to follow these guidelines when creating charts for TASO publications, they are highly flexible according to context.\nThe second section gives more technical details on styling charts for TASO publications. It covers aspects like sizing, font usage and colour schemes. This is to ensure consistency across TASO publications when collaborating with our partners. This section is tailored for TASO staff and partners, so if you are a general audience seeking high-level insights, you may wish to skip over it. However, we hope that general audiences may still find this section helpful if you are interested in developing your own style guide, or in understanding the finer details behind creating charts.\n\n\n\n\n\n\n\nNote\n\n\n\nIn this document, you will find examples of charts that follow our guidelines. These are either redesigns of existing TASO charts using the new guidance, or in some cases new charts to illustrate the guidelines. These charts are only intended to demonstrate how the style guide can be applied, and so should not be taken and shared outside of this document. As we apply the style guide, we will update this document with real examples taken from TASO publications.\n\n\n\nViewing the code\nIn this guide, wherever there is a chart, you will see a ‘Show the code’ button as below. Clicking this will reveal the R code used to create the plot you’re viewing. Using R is not a requirement for making high-quality charts, but for those who use R as their tool of choice, we hope this code will be helpful.\n\n\nShow the code\n# Load the tidyverse package, which includes ggplot2 and other data manipulation tools.\nlibrary(tidyverse)\nlibrary(extrafont) #Note need to run extrafont::font_import() on the R console to import fonts\n\n# Create a dataframe\ndf &lt;- data.frame(\n  X_Axis_Title = c(\"Category 1\", \"Category 2\", \"Category 3\", \"Category 4\"),\n  Y_Axis_Title = c(10, 20, 30, 20)\n)\n\n# Build the plot using ggplot2 package.\nggplot(df, aes(x = X_Axis_Title, y = Y_Axis_Title)) +\n  # Add bar elements to the plot. 'stat = \"identity\"' tells ggplot to use the y-values as they are.\n  geom_bar(stat = \"identity\", fill = \"#3b66bc\") +\n  # Add a horizontal line at y = 0, with a solid line type and a specified color.\n    geom_hline(yintercept = 0, linetype = \"solid\", color = \"#404040\") +\n  # Add labels for the axes, title, subtitle, and caption. Also format these elements.\n  labs(x = \"Axis Title (in italics if included)\", \n       y = \"\", \n       title = \"This is the title of the chart, it should be in Arial bold, no\\nlonger than two lines, and normally active\",\n       subtitle = \"This is the subtitle, it should normally be a formal statistical subtitle\",\n       caption = \"Source: This is the source in italics\\n\\nNotes: These are the notes in italics\") + \n  # Customise with TASO theme elements, including background colour, title positioning, gridlines.\n  theme_minimal() + \n  theme(\n          text = element_text(family = \"Arial\"),\n          plot.title.position = \"plot\",\n          plot.title = element_text(size = 16, face = \"bold\"),\n          plot.subtitle = element_text(size = 12),\n          plot.caption.position = \"plot\",\n          plot.caption = element_text(hjust = 0, size = 8, face = \"italic\"),\n          panel.grid.major.x = element_blank(),\n          panel.grid.major.y = element_line(colour = \"grey\"),\n          plot.background = element_rect(fill = \"#EDEBE3\"),\n          plot.margin = margin(0.25, 0.25, 0.25, 0.25, \"in\"),\n          axis.text.y = element_text(size = 11),\n          axis.text.x = element_text(size = 11),\n          axis.title.x = element_text(size = 10, face = \"italic\"))\n\n\n\n\n\n\n\n\n\n\n\n\nHow we made the guide\nIn developing the guide, we have drawn extensively on existing data visualisation work, notably the key principles outlined by Jonathan Schwabish, and the Government Analysis Function guidance on data visualisation. We aimed to distil and accentuate elements from various sources that we thought would be most beneficial for TASO, our partners and the wider sector. This guide, therefore, stands as a curated synthesis, offering tailored and impactful guidance for our specific context, with due credit to the influential work and expertise that inspired it. We have taken care to strike a balance, ensuring the guide offers practical insights without being overly burdensome. For a comprehensive list of the guides and resources we consulted during this process, please refer to the References and further resources section.",
    "crumbs": [
      "Home",
      "Data visualisation style guide"
    ]
  },
  {
    "objectID": "coding-good-practice/styler.html",
    "href": "coding-good-practice/styler.html",
    "title": "Automatic styling with styleR",
    "section": "",
    "text": "The tidyverse style guide provides all the detail you could need on the specifics of styling code. However, applying it manually can be time consuming. Instead, you can use styler, an R package that helps format your code automatically. If you haven’t already installed it, run install.packages(\"styler\").\nOnce this package is installed (which only needs to be done once), you can use an ‘Addin’ in RStudio. Click the ‘Addins’ button, scroll down to the ‘STYLER’ section and click ‘style active file’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Coding good practice guide",
      "Automatic styling with styleR"
    ]
  },
  {
    "objectID": "coding-good-practice/good_practice.html",
    "href": "coding-good-practice/good_practice.html",
    "title": "Good practice tips",
    "section": "",
    "text": "Efficient and clean coding practices are fundamental for effective data analysis and collaboration. This section offers a set of good practice tips tailored for scripting in R.1 These guidelines are designed to improve the readability, shareability, and functionality of your code.",
    "crumbs": [
      "Home",
      "Coding good practice guide",
      "Good practice tips"
    ]
  },
  {
    "objectID": "coding-good-practice/good_practice.html#tips-for-effective-r-scripting",
    "href": "coding-good-practice/good_practice.html#tips-for-effective-r-scripting",
    "title": "Good practice tips",
    "section": "Tips for effective R scripting",
    "text": "Tips for effective R scripting\n\nStructuring the scripts\n\nUse commented lines of - to help break up the script into readable sections.\nLoad all required packages, then all required files. Only load the packages that are necessary for your script.\n\n# Loading libraries and data --------------------------------------------------\n\n# Load libraries (install if needed)\n\nlibrary(tidyverse) \n\n# Load data \n\ndf &lt;- read.csv(\"example_data_vis.csv\")\n\n# Tidying the data ------------------------------------------------------------\n\n# Filter by year\ndf &lt;- df %&gt;% \n  filter(enrollment_year == 2020)\n\n# Separating a full name into first and last name\ndf_separated &lt;- df %&gt;% separate(col = FullName, \n                                into = c(\"FirstName\", \"LastName\"), \n                                sep = \" \")\n\nInclude a line on who wrote the script, and a line or two on what its purpose is.\n\n# Open University Learner Analytics Data (OULAD) ------------------------------\n\n# 1. OULAD loading and cleaning -----------------------------------------------\n\n# Introduction ----------------------------------------------------------------\n\n# Ongoing document written by LA with support from RJS.\n# Analyses the Open University Learner Analytics Dataset.\n# Info on this open dataset and the data can be found here:\n# https://analyse.kmi.open.ac.uk/open_dataset\n\nIf the script is unwieldy, consider breaking the code up into separate files. As a rule of thumb, a file shouldn’t be longer than 2000 lines.\n\n\n\nCoding practices\n\nWhere possible, use the tidyverse rather than Base R. The tidyverse has friendly syntax and is often easier to read than Base R.2\n\n\n\nComment liberally, explaining what your code is doing and why.\n\n# Creating binary deprivation measure\n# Where bottom three deciles are coded as deprived\nstudent_info &lt;- student_info %&gt;%\n  mutate(deprivation_status = case_when(\n    is.na(imd_decile) ~ NA_character_,\n    imd_decile %in% 1:3 ~ \"Deprived\", \n    imd_decile %in% 4:10 ~ \"Not deprived\", \n    TRUE ~ \"ERROR\"\n  ))\n  \n# student_info has more than one row for many students, who are taking more than one module\n# So we can keep unique id_student to perform a lookup \nstudent_info_unique &lt;- student_info %&gt;%\n  distinct(id_student, .keep_all = TRUE)\n\nTry to keep the code to 80 characters or less per line for readability.\nUse &lt;- not = for assignment.3\n\n\n\nTry not to repeat yourself - if you’re repeating the same code many times, consider using a loop or function, which will save time and decrease the likelihood of making a mistake.\n\n\n\nOrganising files and workspace\n\nAim for names (both for files and variables) that are concise and meaningful.\n\nBad practice:\n\ndata_analysis_version2_final.R\nnew_script_updated.R\nfinal_data_03.04.2024.R\n\nGood practice:\n\nstudent_performance_analysis.R\ncourse_enrollment_trends.R\nfaculty_publication_summary.R\n\n\nIf files should be run in a certain order, begin the file name with numbers (e.g. 1_tidy_data, 2_exploratory_analysis, 3_model).\nIt may be helpful to explicitly specify R script or package dependencies clearly.4 Use a DESCRIPTION file to detail required R packages and their versions. For scripts, consider including a README file that mentions all necessary packages and scripts, ensuring users know exactly what they need to run your code smoothly.\n\n\n\nTry to avoid modifying raw data. If you modify data manually you might make errors, and won’t be able to restore the original version. Creating a script that processes the raw data and creates a new version enables you to keep track of all the steps you have taken.\nStart with a clean environment: do not save the workspace (which is turned on by default).5 Go to Tools -&gt; ‘Global Options’, then select ‘Never’ for ‘Save workspace to .RData on exit’.",
    "crumbs": [
      "Home",
      "Coding good practice guide",
      "Good practice tips"
    ]
  },
  {
    "objectID": "coding-good-practice/good_practice.html#footnotes",
    "href": "coding-good-practice/good_practice.html#footnotes",
    "title": "Good practice tips",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR and RStudio are two different things. R is a programming language for statistics, and RStudio is a tool that makes R easier to use. It’s rare, but someone might have R without RStudio. This guide is written assuming you have both R and RStudio installed.↩︎\nThere is debate around this. However this discussion is beyond the scope of this guide. All that we need to know for the purposes of this guide is that the tidyverse is a collection of packages designed to work together in data science tasks. It simplifies data manipulation, visualisation and analysis through a consistent syntax and logical workflow. For all things tidyverse, see R for Data Science.↩︎\nIn R, &lt;- assigns values to variables, while == tests equality. Using different symbols for assignment and equality makes R code more readable and easier to follow.↩︎\nDependencies in R are external packages or software your code needs to run, such as ggplot2 for graphs or dplyr for data handling. Listing these packages ensures others can use your code without issues, making it easier to share and reproduce your work across different systems.↩︎\nA ‘clean environment’ means having a fresh workspace where no previous work (like variables or data sets you’ve created, or libraries you’ve loaded) is left over. This is crucial for making sure your code runs smoothly and is easy for others to understand. The most straightforward way to achieve a clean environment is by going to the ‘Session’ menu and selecting ‘Restart R’.↩︎",
    "crumbs": [
      "Home",
      "Coding good practice guide",
      "Good practice tips"
    ]
  },
  {
    "objectID": "coding-good-practice/furtherresources.html",
    "href": "coding-good-practice/furtherresources.html",
    "title": "Further resources",
    "section": "",
    "text": "If you are unsure on anything, or if something is not covered here, check the Tidyverse Style Guide.",
    "crumbs": [
      "Home",
      "Coding good practice guide",
      "Further resources"
    ]
  },
  {
    "objectID": "coding-good-practice/furtherresources.html#key-resources",
    "href": "coding-good-practice/furtherresources.html#key-resources",
    "title": "Further resources",
    "section": "Key resources",
    "text": "Key resources\nOther key resources for building this guidance include:\n\nBest Practices for Writing R Code\nR Code - Best practices\nGoogle’s R Style Guide\nProductive R Workflow",
    "crumbs": [
      "Home",
      "Coding good practice guide",
      "Further resources"
    ]
  },
  {
    "objectID": "coding-good-practice/furtherresources.html#feedback",
    "href": "coding-good-practice/furtherresources.html#feedback",
    "title": "Further resources",
    "section": "Feedback",
    "text": "Feedback\nIf you have any best practice tips you think should be included, please get in touch. Your input is invaluable in improving this guide.",
    "crumbs": [
      "Home",
      "Coding good practice guide",
      "Further resources"
    ]
  },
  {
    "objectID": "coding-good-practice/index.html",
    "href": "coding-good-practice/index.html",
    "title": "Coding good practice guide",
    "section": "",
    "text": "Code hygiene is crucial for maintaining the quality and readability of code.1 This document sets out guidelines for ensuring that code is accessible and shareable.",
    "crumbs": [
      "Home",
      "Coding good practice guide"
    ]
  },
  {
    "objectID": "coding-good-practice/index.html#who-is-this-for",
    "href": "coding-good-practice/index.html#who-is-this-for",
    "title": "Coding good practice guide",
    "section": "Who is this for?",
    "text": "Who is this for?\nThis guide is for those who have some experience and familiarity with R and are looking to refine their coding approach.\nPractitioners, academics and students can all gain from this guide. It is particularly beneficial for those dealing with complex scripts, revisiting their code for updates, or sharing work with colleagues and the wider sector.",
    "crumbs": [
      "Home",
      "Coding good practice guide"
    ]
  },
  {
    "objectID": "coding-good-practice/index.html#why-is-good-code-hygeine-important",
    "href": "coding-good-practice/index.html#why-is-good-code-hygeine-important",
    "title": "Coding good practice guide",
    "section": "Why is good code hygeine important?",
    "text": "Why is good code hygeine important?\n\n\n\n\n\n\nHadley Wickham on coding style\n\n\n\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.”\n\n\nGood code hygiene entails writing code that is not only functional but also clean, well-organised, and easy to understand.\nFor non-expert R users, maintaining good code hygiene can simplify the learning curve. Well-written code is easier to debug, update, and share. This guide offers practical tips for immediate application, helping you write clean and readable code.\nBy following these guidelines, you will improve your own coding skills. You will also contribute to the broader research community by making your code more accessible to others. Whether working on personal projects, collaborating with a team, or contributing to open-source software, these guidelines will enhance the quality and longevity of your R code.",
    "crumbs": [
      "Home",
      "Coding good practice guide"
    ]
  },
  {
    "objectID": "coding-good-practice/index.html#footnotes",
    "href": "coding-good-practice/index.html#footnotes",
    "title": "Coding good practice guide",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs R is the primary statistical software used by TASO, this document is written specifically with R in mind. However, many of the guidelines will apply to other languages like Python or Stata.↩︎",
    "crumbs": [
      "Home",
      "Coding good practice guide"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact point",
    "section": "",
    "text": "We view these guides as iterative resources and want to emphasise the valuable role feedback plays in their improvement. Your insights are crucial to refining and enhancing these resources. We encourage you to share your thoughts, suggestions and experiences with us, to ensure the ongoing relevance and effectiveness of these guides. For any inquiries, comments, or feedback, you can contact us at research@taso.org.uk.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Contact point"
    ]
  },
  {
    "objectID": "data-vis/principles.html",
    "href": "data-vis/principles.html",
    "title": "Principles for good data visualisation",
    "section": "",
    "text": "In this section, we present guiding principles to improve your chart design. We encourage TASO partners to implement these principles when designing charts. Applying these principles may vary depending on specific contexts and needs, but keeping them in mind can help make your charts more impactful and communicate your ideas more effectively.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#choosing-your-chart",
    "href": "data-vis/principles.html#choosing-your-chart",
    "title": "Principles for good data visualisation",
    "section": "Choosing your chart",
    "text": "Choosing your chart\nWhen deciding on a chart, remember the core message you intend to convey. If summarising your chart’s point takes more than a few sentences, you may need to reconsider your choice. The Visual Vocabulary tool by the Financial Times is a valuable resource for selecting the right chart and sparking creative ideas.1 Consider your audience when making your selection – what works well in a technical report may not resonate as effectively in a webinar or blog post. Remember, charts are more than data points; they are narratives designed to tell a compelling story, prioritising clarity over exhaustive detail.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#raise-principles---summary",
    "href": "data-vis/principles.html#raise-principles---summary",
    "title": "Principles for good data visualisation",
    "section": "RAISE principles - summary",
    "text": "RAISE principles - summary\nThese principles are adapted from the key steps for better data visualisation set out by Jonathan Schwabish.2 Keep them in mind when designing charts to help make them as engaging as possible:\n\nReduce the clutter: excessive chart elements, such as dense gridlines, or superfluous tick marks and labels, can reduce the effectiveness of a chart.\nAnnotate: consider including explanatory text to help the audience understand how to interpret the visualisation, guiding them through the content.\nIntegrate the text: where feasible, incorporate legends directly into the chart and use active titles that capture the primary takeaway. If articulating the key message is challenging, you may need to reassess your choice of chart.\nShow the data: consider highlighting the values that are more important to your argument, particularly if there is a lot of information on the chart. This helps ensure the audience focuses on the most pertinent data points.\nEngage the audience: thoughtfully assess the chart’s context and consider how to effectively engage the audience. A chart extracted from a technical report may not translate as effectively in a webinar or blog post and may demand adaptation.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#reduce-the-clutter",
    "href": "data-vis/principles.html#reduce-the-clutter",
    "title": "Principles for good data visualisation",
    "section": "Reduce the clutter",
    "text": "Reduce the clutter\nStart with the principle of making charts as simple and stripped back as possible, and gradually incorporate elements as needed. Unnecessary and distracting visual components can compromise the overall effectiveness of your visualisation.\n\nText should almost always be horizontal. Sometimes it may be worth considering the format of the chart to incorporate this, such as horizontal bars instead of vertical.\nConsider if you can label data points directly – if you can label all data points, you may be able to drop axis labels.\nConsider if axis titles need to be included. For example, if the X-axis shows ‘2013, 2014, 2015’, there is no need to add ‘Year’ as the X-axis title.\nConsider how many gridlines are helpful – too few can mean they are not useful, and too many can make the chart cluttered.\nDo not use unnecessary elements like colour gradients or 3D effects.\n\n\nClutteredTidy\n\n\n\n\nShow the code\n# Load the tidyverse package.\nlibrary(tidyverse)\nlibrary(extrafont)\n\n# Create a dataframe.\ndf &lt;- data.frame(\n  category = c(\"Social or communication impairment\", \n                 \"Sensory, medical or physical impairments\", \n                 \"Multiple impairments\", \n                 \"Mental health condition\", \n                 \"Cognitive or learning difficulties\"),\n  proportion = c(0.006, 0.022, 0.026, 0.039, 0.05)\n)\n\n# Build the plot using ggplot2 package.\nggplot(df, aes(x=reorder(category, proportion), y=proportion, fill = category)) +\n  # Add bars to the plot to represent each category's proportion.\n  geom_bar(stat=\"identity\") +\n  # Flip the plot to horizontal layout for better readability of category names.\n  coord_flip() + \n  # Set the labels for the plot, including title, subtitle, axes labels, and caption.\n  labs(x = \"Category\", y = \"Proportion of students (%)\", \n       title = \"The range and nature of disability varies considerably\",\n       subtitle = \"Figure 1: Proportion of students studying in England who declared a disability\\nby impairment, 2018-19\",\n       caption = \"Source: Reproduced from OfS analysis of equality and diversity data\\n\\nNote: 85.7% of students do not declare a disability \n\"\n) + \n  # Format the y-axis as a percentage for better interpretation.\n  scale_y_continuous(labels = scales::percent_format(scale = 100),\n  expand = c(0.02, 0)) +  \n  # Add text labels to each bar for direct reading of percentages.\n  geom_text(aes(label = scales::percent(proportion)), \n            vjust = 0.5, \n            hjust = 1.5, \n            size = 3, \n            fontface = \"bold\", \n            data = df, \n            color = \"white\") +\n\n# Customise with TASO elements.\ntheme_minimal() + \n  # Further customize the plot's appearance including font, title, grid lines, and margins.\n  theme(\n        text = element_text(family = \"Arial\"),\n        plot.title.position = \"plot\",\n        plot.title = element_text(size = 16, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        plot.caption.position = \"plot\",\n        plot.caption = element_text(hjust = 0, size = 8, face = \"italic\"),\n        panel.grid.major = element_line(colour = \"darkgrey\"),\n        panel.grid.minor = element_line(colour = \"darkgrey\"),\n        plot.background = element_rect(fill = \"#EDEBE3\"),\n        plot.margin = margin(0.25, 0.25, 0.25, 0.25, \"in\"),\n        axis.text.x = element_text(size = 11),\n        axis.text.y = element_text(size = 11),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(tidyverse)\n\ndf &lt;- data.frame(\n  category = c(\"Social or communication impairment\", \n                 \"Sensory, medical or physical impairments\", \n                 \"Multiple impairments\", \n                 \"Mental health condition\", \n                 \"Cognitive or learning difficulties\"),\n  proportion = c(0.006, 0.022, 0.026, 0.039, 0.05)\n)\n\nggplot(df, aes(x=reorder(category, proportion), y=proportion)) +\n  geom_bar(stat=\"identity\", fill = \"#3b66bc\") +\n  coord_flip() + # Flip the coordinates to make the bars horizontal\n  labs(x = \"\", y = \"\", \n       title = \"The range and nature of disability varies considerably\",\n       subtitle = \"Figure 1: Proportion of students studying in England who declared a disability\\nby impairment, 2018-19\",\n       caption = \"Source: Reproduced from OfS analysis of equality and diversity data\\n\\nNote: 85.7% of students do not declare a disability \n\"\n) + \nscale_y_continuous(labels = scales::percent_format(scale = 100),\nexpand = c(0.02, 0)) +  # Formatting axis as percentage\ngeom_text(aes(label = scales::percent(proportion)), \n          vjust = 0.5, \n          hjust = 1.5, \n          size = 3, \n          fontface = \"bold\", \n          data = df, \n          color = \"white\") +\n  theme_minimal() + \n  theme(\n        text = element_text(family = \"Arial\"),\n        plot.title.position = \"plot\",\n        plot.title = element_text(size = 16, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        plot.caption.position = \"plot\",\n        plot.caption = element_text(hjust = 0, size = 8, face = \"italic\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(\"grey\"),\n        plot.background = element_rect(fill = \"#EDEBE3\"),\n        plot.margin = margin(0.25, 0.25, 0.25, 0.25, \"in\"),\n        axis.text.x = element_text(size = 11),\n        axis.text.y = element_text(size = 11))",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#annotate",
    "href": "data-vis/principles.html#annotate",
    "title": "Principles for good data visualisation",
    "section": "Annotate",
    "text": "Annotate\nConsider using annotations if they will help tell the story of the data. They can be used to highlight key information and support your analysis, bridging the gap between raw data and interpretation, and guiding the reader’s thinking process. They can also add context, if there are outliers, peaks or troughs, or if the chart type is unfamiliar to the reader.\n\nRelevance is key – ensure that annotations directly contribute to the narrative of the data. Avoid unnecessary or excessive annotations that could confuse the reader. Use them selectively to emphasise key insights.\nMake annotations concise, and place them near the relevant data points.\nEnsure they are distinct from other elements of the chart, such as through font size or weight.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#integrate-the-text",
    "href": "data-vis/principles.html#integrate-the-text",
    "title": "Principles for good data visualisation",
    "section": "Integrate the text",
    "text": "Integrate the text\n\nActive titles\nThe choice of a chart title is often contextual, influenced by factors such as the nature of the content and any constraints posed by technical reports or academic papers. In general, opt for an active title that succinctly captures the key takeaway, complemented by a formal statistical subtitle. Titling charts in this way can make them more memorable and easier to digest. For more on why we think this is a more effective way to title charts, please see this blog post by the Office for National Statistics.3\n\n\n\n\n\n\nExample of best practice for chart titles and subtitles\n\n\n\nTitle: Students with mental health difficulties consistently have the highest anxiety\nSubtitle: Figure 18: Average anxiety level over time by disability type\n\n\n\n\n\n\n\n\nThis is traditionally the most common way of titling charts, but can be less helpful than an active title\n\n\n\nTitle: Fig. 18: Average anxiety level over time by disability type\n\n\n\n\nIntegrate the legend\nIntegrating the legend into the chart aids reader interpretation, creating a smoother, more intuitive experience. With an integrated legend, viewers do not need to shift their focus between the visualisation and a separate legend, which can be particularly difficult when there are multiple, similar colours involved.\n\nStandardIntegrated legend\n\n\n\n\nShow the code\nlibrary(tidyverse)\n\ntaso_three_colour &lt;- c(\"#3b66bc\", \"#00a8da\", \"#07dbb3\")\n\ndf &lt;- data.frame(\n  Year = c(2019, 2020, 2021, 2022),\n  School_A = c(10, 12, 15, 13),\n  School_B = c(17, 15, 21, 24),\n  School_C = c(4, 2, 5, 6)\n)\n\ndf &lt;- gather(df, key = \"School\", value = \"Value\", -Year)\n\n# Now you can create a plot with ggplot\nggplot(df, aes(x = Year, y = Value, color = School, group = School)) +\n  geom_line() + # or geom_point() if you want points\n  geom_point() +\n  scale_colour_manual(values = taso_three_colour) +\n  theme_minimal() +\n  theme(\n          text = element_text(family = \"Arial\"),\n          panel.grid.major.x = element_line(colour = \"#E4E2D9\"),\n          panel.grid.major.y = element_line(colour = \"#E4E2D9\"),\n          plot.background = element_rect(fill = \"#EDEBE3\"),\n          plot.margin = margin(0.25, 0.25, 0.25, 0.25, \"in\"),\n          axis.text.y = element_text(size = 11),\n          axis.text.x = element_text(size = 11), \n          axis.title = element_blank()) +\n  coord_cartesian(clip = \"off\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(tidyverse)\n\ntaso_three_colour &lt;- c(\"#3b66bc\", \"#00a8da\", \"#07dbb3\")\n\ndf &lt;- data.frame(\n  Year = c(2019, 2020, 2021, 2022),\n  School_A = c(10, 12, 15, 13),\n  School_B = c(17, 15, 21, 24),\n  School_C = c(4, 2, 5, 6)\n)\n\ndf &lt;- gather(df, key = \"School\", value = \"Value\", -Year)\n\n# Now you can create a plot with ggplot\nggplot(df, aes(x = Year, y = Value, color = School, group = School)) +\n  geom_line() + # or geom_point() if you want points\n  geom_point() +\n  geom_text(data = subset(df, Year == 2022), aes(label = School), \n            hjust = 0.3, vjust = -1, size = 3.5, fontface = \"bold\") +\n  scale_colour_manual(values = taso_three_colour) +\n  theme_minimal() +\n  theme(\n          text = element_text(family = \"Arial\"),\n          panel.grid.major.x = element_line(colour = \"#E4E2D9\"),\n          panel.grid.major.y = element_line(colour = \"#E4E2D9\"),\n          plot.background = element_rect(fill = \"#EDEBE3\"),\n          plot.margin = margin(0.25, 0.25, 0.25, 0.25, \"in\"),\n          axis.text.y = element_text(size = 11),\n          axis.text.x = element_text(size = 11), \n          axis.title = element_blank()) +\n  coord_cartesian(clip = \"off\") + \n  guides(fill = FALSE, colour = FALSE)",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#show-the-data",
    "href": "data-vis/principles.html#show-the-data",
    "title": "Principles for good data visualisation",
    "section": "Show the data",
    "text": "Show the data\nConsider highlighting the values that are most important to your argument. You don’t need to show all the data all of the time, as this can make it hard to see the data that matters most. For example, for line charts, try to have no more than four or five different colours.\nYou can change the properties of parts of the chart – such as by selectively using colour, shape and saturation – to guide a reader’s attention and make parts of a chart stand out.\n\nStandardHighlighting data\n\n\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggtext)\n\ndf &lt;- data.frame(\n  category = c(\"No\", \n               \"Yes - inadequately specified\", \n               \"Yes - explores the general approach\", \n               \"Yes - adequately specified (institutional level)\", \n               \"Yes - adequately specified (intervention level)\"),\n  count = c(42, 31, 17, 26, 27)\n)\n\nggplot(df, aes(x = count, y = factor(category, levels = c(\"No\", \n                                                                         \"Yes - inadequately specified\", \n                                                                         \"Yes - explores the general approach\", \n                                                                         \"Yes - adequately specified (institutional level)\", \n                                                                         \"Yes - adequately specified (intervention level)\")))) +\n  # ^ sorting the bars in the chart in desired order\n  geom_bar(stat = \"identity\", fill = \"#EDEBE3\") + # Adding bars of same colour as background underneath so alpha can be used for colouring\n  geom_bar(stat = \"identity\", fill = \"#3b66bc\") + # Bar chart coloured peacock\n  theme_minimal() + # Setting the theme as minimal, adding elements we want back in \n  # Title, subtitle, axis titles and caption \n  labs(\n    title = 'Over 40 providers did not include a Theory of Change  \\n(ToC) in their Access and Participation Plan (APP)',\n    # By using ggtext::element_markdown, we can use markdown stylings to colour specific words in the chart to highlight a point\n    subtitle = \"Figure 3: The number of Higher Education Providers (HEPs) who included a\\nToC in their APP\",\n    caption = \"Source: TASO (2023), Approaches to addressing the ethnicity degree awarding gap\",\n    x = NULL, # Not showing X or Y axis title for this example chart\n    y = NULL \n  ) +\n  scale_x_continuous(expand = c(0,0)) + \n  geom_vline(xintercept = 0, colour = \"#404040\", linewidth = 0.5) +\n  # Theme elements\n  theme(text = element_text(family = \"Arial\"), # Specifying font for whole chart\n        ## Title and subtitle elements \n        plot.title.position = \"plot\", # Aligning title to entire plot (not just panel)\n        plot.title = ggtext::element_markdown(face = \"bold\", \n                                              size = 16), \n        plot.subtitle = element_text(size = 12), \n        ## Caption elements \n        plot.caption.position = \"plot\", # Aligning caption to entire plot\n        plot.caption = element_text(hjust = 0, # Aligning font to the left\n                                    size = 8, # Changing size\n                                    face = \"italic\"), # Italicizing caption\n        ## Other theme elements\n        plot.background = element_rect(fill = \"#EDEBE3\"), # Changing background colour\n        plot.margin = margin(0.25, 0.25, 0.25, 0.25, \"in\"), # Setting margin in inches\n        axis.text.x = element_text(size = 10), \n        panel.grid.major.x = element_line(colour = \"grey\"),\n\n  ) + \n  # Other helpful elements\n  coord_cartesian(clip = \"off\") +\n  geom_text(aes(label = count, colour = category), vjust = 0.5, hjust = 1.5, size = 3, fontface = \"bold\", \n            data = df, color = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggtext)\n\ndf &lt;- data.frame(\n  category = c(\"No\", \n               \"Yes - inadequately specified\", \n               \"Yes - explores the general approach\", \n               \"Yes - adequately specified (institutional level)\", \n               \"Yes - adequately specified (intervention level)\"),\n  count = c(42, 31, 17, 26, 27)\n)\n\nggplot(df, aes(x = count, y = factor(category, levels = c(\"No\", \n                                                                         \"Yes - inadequately specified\", \n                                                                         \"Yes - explores the general approach\", \n                                                                         \"Yes - adequately specified (institutional level)\", \n                                                                         \"Yes - adequately specified (intervention level)\")))) +\n  # ^ sorting the bars in the chart in desired order\n  geom_bar(stat = \"identity\", fill = \"#EDEBE3\") + # Adding bars of same colour as background underneath so alpha can be used for colouring\n  geom_bar(stat = \"identity\", fill = \"#3b66bc\", alpha = 0.4) + # Bar chart coloured peacock\n  theme_minimal() + # Setting the theme as minimal, adding elements we want back in \n  # Title, subtitle, axis titles and caption \n  labs(\n    title = 'Over 40 providers &lt;span style=\"color:#3b66bc;\"&gt;did not include a Theory of Change&lt;/span&gt;  \\n(ToC) in their Access and Participation Plan (APP)',\n    # By using ggtext::element_markdown, we can use markdown stylings to colour specific words in the chart to highlight a point\n    subtitle = \"Figure 3: The number of Higher Education Providers (HEPs) who included a\\nToC in their APP\",\n    caption = \"Source: TASO (2023), Approaches to addressing the ethnicity degree awarding gap\",\n    x = NULL, # Not showing X or Y axis title for this example chart\n    y = NULL \n  ) +\n  scale_x_continuous(expand = c(0,0)) + \n  geom_vline(xintercept = 0, colour = \"#404040\", linewidth = 0.5) +\n  # Theme elements\n  theme(text = element_text(family = \"Arial\"), # Specifying font for whole chart\n        ## Title and subtitle elements \n        plot.title.position = \"plot\", # Aligning title to entire plot (not just panel)\n        plot.title = ggtext::element_markdown(face = \"bold\", \n                                              size = 16), \n        plot.subtitle = element_text(size = 12), \n        ## Caption elements \n        plot.caption.position = \"plot\", # Aligning caption to entire plot\n        plot.caption = element_text(hjust = 0, # Aligning font to the left\n                                    size = 8, # Changing size\n                                    face = \"italic\"), # Italicizing caption\n        ## Other theme elements\n        plot.background = element_rect(fill = \"#EDEBE3\"), # Changing background colour\n        plot.margin = margin(0.25, 0.25, 0.25, 0.25, \"in\"), # Setting margin in inches\n        axis.text.x = element_text(size = 10), \n        panel.grid.major.x = element_line(colour = \"grey\"),\n\n  ) + \n  # Other helpful elements\n  coord_cartesian(clip = \"off\") + \n  geom_bar(data = subset(df, category == \"No\"), fill = \"#3b66bc\", stat = \"identity\", show.legend = FALSE) + \n  geom_text(aes(label = count, colour = category), vjust = 0.5, hjust = 1.5, size = 3, fontface = \"bold\", \n            data = subset(df, category == \"No\"), color = \"white\") +\n  geom_text(aes(label = count, colour = category), vjust = 0.5, hjust = 1.5, size = 3, \n            data = subset(df, category != \"No\"), color = \"black\")",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#engage-the-audience",
    "href": "data-vis/principles.html#engage-the-audience",
    "title": "Principles for good data visualisation",
    "section": "Engage the audience",
    "text": "Engage the audience\nStrive to make your visualisation as engaging as possible. Capturing the reader’s attention ensures that your message is not only seen but also comprehended and remembered. Engaging visuals spark curiosity, encouraging a deeper exploration and analysis of the data. This connection leads to a more profound understanding of the insights you are sharing.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#guidance-for-common-chart-types",
    "href": "data-vis/principles.html#guidance-for-common-chart-types",
    "title": "Principles for good data visualisation",
    "section": "Guidance for common chart types",
    "text": "Guidance for common chart types\n\nBar charts\n\nBar charts should always start at zero.\nOrder bars by their values, unless there is a logical order, such as level of qualification or date.\nConsider directly labelling data points and removing gridlines if there are only a few bars in your chart.\nAs text should be horizontal, a horizontal bar chart may be more appropriate when your axis labels are long.\n\n\n\nLine charts\n\nTry not to use more than five lines on a single chart. Including too many lines can make the chart confusing, particularly when they cross over at multiple points.\n\nIf you need to include more than this, consider splitting your chart into small multiples (also known as faceting or panel charts).\nAlternatively, consider highlighting selected lines of interest and greying the rest out. This makes clear the line of interest while still enabling readers to easily see how it compares to other lines.\n\nTry to integrate the legend and label lines directly, so readers do not have to match the colours from the line to the legend, as in the example on the right below.\n\n\n\nPie charts\n\nGenerally, we find that bar charts are more effective than pie charts, as it can be hard to discern the difference in size between different segments.\nPie charts can be particularly difficult to read when values are close, or if there are multiple segments.\nConsider whether another chart type may be more appropriate. However, this is a rule of thumb and pie charts can be used, but do so with caution.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/principles.html#footnotes",
    "href": "data-vis/principles.html#footnotes",
    "title": "Principles for good data visualisation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFinancial Times. 2019. Visual vocabulary. https://ft-interactive.github.io/visualvocabulary/ [Accessed 21 November 2023]↩︎\nSchwabish, J. 2021. Better data visualizations: A guide for scholars, researchers, and wonks. Columbia University Press.↩︎\nDonnarumma, F. 2019. Say what you see - the way we write chart titles is changing. https://digitalblog.ons.gov.uk/2019/01/28/say-what-you-see-the-way-we-write-charttitles-is-changing/ [Accessed 21 November 2023]↩︎",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Principles for good data visualisation"
    ]
  },
  {
    "objectID": "data-vis/styling.html",
    "href": "data-vis/styling.html",
    "title": "Styling charts for TASO publications",
    "section": "",
    "text": "This section is designed with TASO partners and staff in mind, providing rules on chart formatting to ensure consistency across TASO outputs. That said, we anticipate that the insights provided may also be beneficial to anyone interested in developing their own data visualisation style guide, or understanding the technical details behind creating consistently formatted and distinctive charts.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Styling charts for TASO publications"
    ]
  },
  {
    "objectID": "data-vis/styling.html#formatting-rules",
    "href": "data-vis/styling.html#formatting-rules",
    "title": "Styling charts for TASO publications",
    "section": "Formatting rules",
    "text": "Formatting rules\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(extrafont)\n\ndf &lt;- data.frame(\n  X_Axis_Title = c(\"Category 1\", \"Category 2\", \"Category 3\", \"Category 4\"),\n  Y_Axis_Title = c(10, 20, 30, 20)\n)\n\nggplot(df, aes(x = X_Axis_Title, y = Y_Axis_Title)) +\n  geom_bar(stat = \"identity\", fill = \"#3b66bc\") +\n    geom_hline(yintercept = 0, linetype = \"solid\", color = \"#404040\") +\n  labs(x = \"Axis Title (in italics if included)\", \n       y = \"\", \n       title = \"This is the title of the chart, it should be in Arial bold, no\\nlonger than two lines, and normally active\",\n       subtitle = \"This is the subtitle, it should normally be a formal statistical subtitle\",\n       caption = \"Source: This is the source in italics\\n\\nNotes: These are the notes in italics\") + \n  theme_minimal() + \n  theme(\n          text = element_text(family = \"Arial\"),\n          plot.title.position = \"plot\",\n          plot.title = element_text(size = 16, face = \"bold\"),\n          plot.subtitle = element_text(size = 12),\n          plot.caption.position = \"plot\",\n          plot.caption = element_text(hjust = 0, size = 8, face = \"italic\"),\n          panel.grid.major.x = element_blank(),\n          panel.grid.major.y = element_line(colour = \"grey\"),\n          plot.background = element_rect(fill = \"#EDEBE3\"),\n          plot.margin = margin(0.25, 0.25, 0.25, 0.25, \"in\"),\n          axis.text.y = element_text(size = 11),\n          axis.text.x = element_text(size = 11),\n          axis.title.x = element_text(size = 10, face = \"italic\"))\n\n\n\n\n\n\n\n\n\nYou can find an example R script for creating charts with TASO styling on our GitHub. R is a widely used statistical software among researchers and data analysis. You can also find useful resources for data visualisation in R in the R resources section.\n\nThe font for the charts is Arial (or URW DIN if you have access). Ensure that font sizes always remain readable. If there is text you must include but you are finding it is too small, consider how you can redesign the chart. For annotations and data labels, if you are to use bolding or italics, use them with purpose.\nThe title should normally be an active title that conveys the key takeaway and should always be the largest text on the page.\nThe subtitle should be a formal statistical title (as you might traditionally title a chart).\nThe caption should be in the bottom left of the chart. It should be in italics, and smaller than the rest of the text on the chart.\nText sizes can have some flexibility. But ensure that the title is the largest text on the page, followed by the subtitle. Ensure all other text is smaller than the title and subtitle.\nColours\n\nThe text should be black, but adjust as appropriate for readability. For example, sometimes data labels will need to be white to stand out against colours.\nBackground should be #EDEBE3 (putty).\nGridlines should generally be #E4E2D9 (darker than the background but lighter than text and lines). Gridline shade can be adjusted if it aids readability, but it should be a shade of grey.\n\nFormat\n\n180mm (width) x 120mm (height) is the default standard chart size for A4 PDF reports. 180mm is the maximum width, however, the height is flexible depending on the chart. For example, if your chart needs to be tall rather than wide.\nSave as a PNG or a JPEG to avoid losing quality through copy and pasting or screenshots.\n\nIf you are including the logo, it should be in the bottom right corner, with sufficient space around it as per the brand guidelines. The logo is not needed in PDF reports that already contain TASO branding. However, consider including the logo for web/social media versions of charts, where the chart is taken outside of the context of the report.\n\n\n\n\n\n\n\nAdjusting charts for different publication types\n\n\n\nWe are currently developing how charts should be styled across different publications. A chart published in an analytical report will require a slightly different set up to a chart for a blog or a social media. TASO are exploring how to optimise charts for each of these mediums. To see how we have tackled this most recently, we can look at charts for TASO’s Education pathways report.\n\nChart for reportChart for social media\n\n\nFor a formal report, we felt it was more appropriate to have the figure caption in text. We still used our ‘active title’ for the figure caption, but we used it in-text, rather than as part of the image itself.\n\n\n\nTo share this chart on social media, we incorporated this figure caption into the chart itself. We also dropped the figure number and the footnotes.",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Styling charts for TASO publications"
    ]
  },
  {
    "objectID": "data-vis/styling.html#brand-colours",
    "href": "data-vis/styling.html#brand-colours",
    "title": "Styling charts for TASO publications",
    "section": "Brand colours",
    "text": "Brand colours\n#07dbb3 #3b66bc #e4e2d9 #edebe3 #f9466c #485866\n\n\n\n\n\n\nNote\n\n\n\nYou can use RColourBrewer to generate gradients in R. Alternatively, you can use this website to modify the brightness and colour intensity and choose the number of colours you want to generate.\n\n\n\n\nShow the code\ntaso_one_colour &lt;- \"#3b66bc\"\ntaso_two_colour &lt;- c(\"#3b66bc\", \"#07dbb3\")\ntaso_three_colour &lt;- c(\"#3b66bc\", \"#00a8da\", \"#07dbb3\")\ntaso_four_colour &lt;- c(\"#3b66bc\", \"#0093d9\", \"#00bad2\", \"#07dbb3\")\ntaso_five_colour &lt;- c(\"#3b66bc\", \"#0089d6\", \"#00a8da\", \"#00c3cc\", \"#07dbb3\")\n\ntaso_likert &lt;- c(\"#f9466c\", \"#fea3ac\", \"#f1f1f1\", \"#9ea8d7\", \"#3b66bc\")",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Styling charts for TASO publications"
    ]
  },
  {
    "objectID": "data-vis/styling.html#accessibility",
    "href": "data-vis/styling.html#accessibility",
    "title": "Styling charts for TASO publications",
    "section": "Accessibility",
    "text": "Accessibility\nEnsure foreground and background colours contrast enough to pass the WCAG AA standard. You can check the colour contrast here.\nFor more in-depth guidance on accessibility, we recommend the accessibility sections of the Government Analysis Function data visualisation guidance and the Royal Statistical Society data visualisation guide.\nAt TASO, we will be looking further into how we can improve the accessibility of our charts, particularly around the use of accessible colour palettes.\nFor alt text, Amy Cesal provides some useful guidance on how to write alt text for charts.1 She includes the following formula to help:\nChart type of type of data where reason for including chart\n\n\n\n\n\n\nAlt text example\n\n\n\nBar chart of the average earnings of students by highest qualification where students who went to a top third university earn the most",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Styling charts for TASO publications"
    ]
  },
  {
    "objectID": "data-vis/styling.html#footnotes",
    "href": "data-vis/styling.html#footnotes",
    "title": "Styling charts for TASO publications",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCesal, A. 2020. Writing Alt Text for Data Visualization. https://medium.com/nightingale/writing-alt-text-for-data-visualization-2a218ef43f81 [Accessed 21 November 2023]↩︎",
    "crumbs": [
      "Home",
      "Data visualisation style guide",
      "Styling charts for TASO publications"
    ]
  },
  {
    "objectID": "dig/barriers.html",
    "href": "dig/barriers.html",
    "title": "Barriers and facilitators of effective evaluation",
    "section": "",
    "text": "EvaluatorsPractitionersSenior managementStaff with responsibility for the APP",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Barriers and facilitators"
    ]
  },
  {
    "objectID": "dig/barriers.html#barriers-to-effective-evaluation",
    "href": "dig/barriers.html#barriers-to-effective-evaluation",
    "title": "Barriers and facilitators of effective evaluation",
    "section": "Barriers to effective evaluation",
    "text": "Barriers to effective evaluation\nTable 1 presents an overview of the barriers to effective evaluation. Key challenges include data inconsistencies, fragmented infrastructure, and a lack of alignment between institutional goals and evaluation processes. These barriers hinder the ability to gather, interpret, and utilise data effectively. These issues contribute to difficulties in maintaining consistent and accurate evaluation practices.\n\n\nEthics/PrivacyOrganisationalData infrastructureData\n\n\n\n\n\n\nTable 1: Barriers to effective evaluation\n\n\n\n\n\n\n\n\n\n\n\n\nIssue\nEffect\nMitigation(s)\n\n\n\n\n\nData is not clearly defined so across the institution there is no consistent understanding of what the data means\nData is interpreted differently in different institutional contexts, e.g. if the definition of commuter student differs across the institution then data from different courses will not be comparable\nData dictionary: central source of fact for what data is recorded and how it is defined\n\n\n\nFragmented data infrastructure means complete dataset requires export from many systems\nTime-consuming to obtain all the data which requires manual joining, increasing the chance of data errors\nA system that collates data from all the institution’s data stores and acts as the central repository\n\n\n\nFragmented data infrastructure where the same or similar data is captured in multiple systems\nData doesn’t always agree between systems meaning similar analysis can lead to different results\nA system that collates data from all the institution’s data stores and acts as the central repository\n\n\n\nFragmented data infrastructure such that different systems have different data owners\nHinders access to data and may mean the organisation doesn’t have a complete picture of the data captured\nA system that collates data from all the institution’s data stores and acts as the central repository\n\n\n\nPolicy change affects what data is captured or how it is captured\nData that is needed for evaluation is not collected or collected differently\nRegister of data users so data that is required for specific purposes not directly specified in legislation continues to be captured\n\n\n\nData definition changes over time, e.g. attendance (previously just lectures) now includes seminars, lab-classes etc.\nHinders longitudinal comparisons\nWhen definitions change, consider how that affects the interpretation and whether data should also continue to be captured using legacy definitions.\n\n\n\nData is insufficient without context, e.g. attendance without knowing what the expected attendance is for that student\nMakes cross-course comparisons difficult\nConsider the differing course requirements and identify data that require context, e.g. expected levels of attendance, or for online submissions expected submission requirements.\n\n\n\nStudent engagement with support activities is not recorded\nEffectiveness of support services or the signposting/referral to them cannot be determined\nConsider using a tracking system or learning analytics system to record access but note privacy implications of such data\n\n\n\nData infrastructure changes mid APP cycle\nData previously captured is not captured\nRegister of data users so data that is required for specific purposes continues to be captured\n\n\n\nData infrastructure changes mid APP cycle\nData definition change for the same data name\nData dictionary: Central source of fact for what data is recorded and how it is defined.\n\n\n\nPoor institutional memory due to staff churn means the context of the institution at different times isn’t understood\nContext of data that isn’t recorded is lost, e.g. no-detriment policies during COVID-19\nData like this is hard to record formally. When anomalies show up in data such as temporary narrowing or widening of equality gaps or changes in attainment, talk to staff who were involved at the time.\n\n\n\nLack of data-access provision\nData is collected but not used to evaluate service performance\nFormalise access to data through an ethics process with protections in place for data subjects.\n\n\n\nLack of understanding of APP and its regulatory importance at executive level\nResources and structures necessary for evaluation are not supported within the university\nSenior staff responsible for APP should have a good understanding of evaluation methodologies extending to resourcing needs, e.g. staff, time, data.\n\n\n\nLack of understanding of evaluation in senior management\nResources and structures necessary for evaluation are not available\nTraining, or recruitment of student success managers who understand evaluation methodologies.\n\n\n\nLack of understanding of APP and its regulatory importance in the planning team\nData isn’t captured or changes to infrastructure are made without reference to APP\nSenior staff responsible for APP should be feed into governance of IT infrastructure/planning team\n\n\n\nAPP cycle is out of step with other institutional reporting cycles (e.g. Teaching Excellence Framework, Research Excellence Framework) and its importance is diminished\nData infrastructure is primarily arranged around TEF or REF reporting leading to changes that affect APP but\nSenior staff responsible for APP should be able to feed into governance of the digital infrastructure/planning team\n\n\n\nLack of a clear ethics process for analysing data prevents evaluation of student support initiatives\nReduces incentives among institutional statistical experts to analyse data to understand and tackle equality gaps in the institution\nEthics governance process sets out clear guidelines for working with secondary data",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Barriers and facilitators"
    ]
  },
  {
    "objectID": "dig/barriers.html#facilitators-of-effective-evaluation",
    "href": "dig/barriers.html#facilitators-of-effective-evaluation",
    "title": "Barriers and facilitators of effective evaluation",
    "section": "Facilitators of effective evaluation",
    "text": "Facilitators of effective evaluation\nIn contrast to Table 1, Table 2 highlights best practices that can support institutions in overcoming these challenges. These facilitators include centralised access to data, consistent systems for recording post-entry interventions, and the establishment of clear ethical guidelines for data analysis. Tools like learning analytics systems and \\ data dashboard with sufficient granularity can help identify and address equality gaps. A shared understanding of data, coupled with dedicated ethics processes and privacy notices, can improve the effectiveness of evaluation.\n\n\nEthics/PrivacyOrganisationalData infrastructureData\n\n\n\n\n\nTable 2: Facilitators of effective evaluation\n\n\n\n\n\n\nFacilitator\nExamples\n\n\n\n\n\nCentralised access to data\nLearning analytics system that collates institutional data\n\n\n\nDedicated system for recording details of post-entry support\nInstitutional CRM system, tracking services, e.g. Higher Education Access Tracker (HEAT) or East Midlands Widening Participation Research and Evaluation Partnership (EMWPREP) that utilise the post-entry MOAT\n\n\n\nConsistent system for recording post-entry interventions\nTypology of post-entry student support interventions; e.g. post-entry MOAT\n\n\n\nShared understanding of what data means\nData Dictionary\n\n\n\nA dedicated ethics process for the analysis of secondary data\nUniversity of Wolverhampton Ethics Process\n\n\n\nPrivacy notice that informs students that institutional data will be used for research and evaluation purposes\nOpen University privacy notice\n\n\n\nPrivacy notice that informs students that data associated with their engagement with student university may be stored on external systems (e.g. HEAT) for research purposes.\nNTU CenSCE extra-curricular and co-curricular activity participant privacy notice",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Barriers and facilitators"
    ]
  },
  {
    "objectID": "dig/diagnose.html",
    "href": "dig/diagnose.html",
    "title": "Diagnose: Identifying equality gaps using institutional data",
    "section": "",
    "text": "DiagnoseEvaluatorsPlanning teamsPractitionersSenior management\nIdentifying equality gaps in your institution is a core part of the APP and Teaching Excellence Framework (TEF) processes. Here, we go above and beyond the OfS guidance and data dashboards to help providers have a deeper understanding of where equality gaps may lie in their institution.\nThe degree to which you can identify and understand equality gaps will depend on the data you have available. In this guide we define two broad classes of data, the learner context and the institution context:\nIn Table 1 we also define three levels of data: Foundation, Intermediate, and Advanced. Although the levels of data are segregated, it is likely that institutions will have data from more than one of these levels. Data in each additional level provides increasing context for the learner and the institution. Not all of the data listed in Table 1 will be useful for identifying equality gaps, but may be useful in understanding what is required from an intervention to reduce equality gaps.\nTable 1: Example data for different levels and contexts of institutional data\n\n\n\n\n\n\n\n\n\n\n\n\n\nContext\n\n\n\nLevel\nLearner\nInstitution\n\n\n\n\nFoundation\nDemographics; for example\nCourse studied\n\n\n- Sex\nModules studied\n\n\n- Ethnicity\n\n\n\n- Disability status\n\n\n\n- Prior attainment\n\n\n\n- Contextual offer holder\n\n\n\n- Area-based measures (IMD, POLAR4, TUNDRA)\n\n\n\nEnd-of-stage grades\n\n\n\nContinuation\n\n\n\nIntermediate\nEngagement data\nCourse contact hours\n\n\nAttendance\nContact hours split by module\n\n\nVirtual learning environment usage\nExtra curricular contact hours\n\n\nLibrary use\n\n\n\nCourse materials access\n\n\n\nExtenuating circumstances\n\n\n\nModule marks\n\n\n\nAdvanced\nEngagement with extra-curriculars\nTimetable of contact hours\n\n\nEngagement with support\nAssessment deadlines\n\n\nPersonal tutor\nModule prerequisites (for example, passing a prior module/assessment required to sit this module)\n\n\nAcademic skills\nSupport services available to the student\n\n\nPastoral support\nPersonal tutor\n\n\nLeave of absence\nAcademic skills\n\n\nAssessment submission data (timing, attempts)\nPastoral support\n\n\nUse of reading lists\n\n\n\nWithin-module assessment grades",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Diagnose"
    ]
  },
  {
    "objectID": "dig/diagnose.html#identifying-gaps-in-the-student-experience",
    "href": "dig/diagnose.html#identifying-gaps-in-the-student-experience",
    "title": "Diagnose: Identifying equality gaps using institutional data",
    "section": "Identifying gaps in the student experience",
    "text": "Identifying gaps in the student experience\n\n\nEvaluatorsPlanning teams\n\n\nTo effectively identify gaps in the student experience, we can use a range of statistical techniques, as outlined in Table 2. This begins with descriptive statistics, which give a foundational understanding of the data by summarising and describing key features, such as counts and measures of average.\nFor a more in-depth statistical analysis, inferential techniques like t-tests and ANOVAs are useful. T-tests enable the comparison of means between two groups, highlighting statistically significant differences. ANOVAs extend this comparison to multiple groups.\nRegression is a powerful tool for those who want to quantify the effect of various factors on student outcomes and explore in more detail the relationship between these factors.\n\n\n\n\n\n\nNote\n\n\n\nHere, we use the Open University Learning Analytics Dataset (OULAD). This analysis is educational and illustrative, to show the potential applications of various statistical techniques, using a dataset with variables familiar to many institutions. For those interested in a more comprehensive and formal analysis of the OULAD, we will be releasing a technical report later this year.\nThe R code to create figures and tables below is available on our GitHub page. We will also be adding step-by-step guidance for performing these analyses in Excel (e.g. like our previous guidance on conducting a paired t-test using Excel).\n\n\n\n\n\n\nTable 2: Statistical methods for identifying gaps in the student experience\n\n\n\n\n\n\nMethod\nPurpose\nFocus\nExample\n\n\n\n\nDescriptive statistics\nSummarise and describe the basic features of the dataset.\nAverages (mean, median, mode), variability (standard deviation, range).\nDisadvantaged students score X% on average, while their more advantaged peers score Y% on average.\n\n\nT-test\nCompare means between two groups.\nAssess if there is a statistically significant difference between the means of two groups.\nIs there a statistically significant different between the attainment of two groups?; e.g. comparing the attainment of disadvantaged students with the attainment of more advantaged students students.\n\n\nAnalysis of Variance (ANOVA)\nCompare means across multiple groups.\nDetermine if there are statistically significant mean differences across three or more groups.\nIs there a statistically significant difference in attainment across more than two groups?; e.g. comparing the score of students by their highest previous qualification (Below A Level vs A Level vs higher education qualification). Note: If the ANOVA revealed a significant difference\n\n\nRegression\nExplore relationships between variables and predict outcomes.\nIdentify the impact of predictor variables on an outcome variable while controlling for variability due to other factors, assess the strength and direction of relationships, and make predictions based on the model.\nIs there a statistically significant effect of disadvantage and gender on attainment when controlling for prior attainment and course studied?\n\n\n\n\n\n\n\n\n\n\n\nDescriptive analysis\nIdentifying performance gaps among different student groups is important. It allows us to understand how various factors like socio-economic status, gender and disability contribute to disparities in outcomes. Recognising these gaps is the first step to developing interventions that can mitigate the effects of such disparities.\nFor example, if we believe that there is likely to be a gap in attainment between students from deprived areas (IMD quintile 1 and 2) and students who are not from deprived areas (IMD quintiles 3 to 5), we might start by visualising the average attainment in terms of module scores. Using a box plot, as in Figure 1, helps us to see how the distribution of scores differs between these groups.\n\n\n\nFigure 1: Deprivation gaps\n\n\n\n\n\n\n\n(a) Average scores are lower for students from deprived areas\n\n\n\n\n\n\n\n\n\n\n\n(b) Across modules, deprived students score lower on average\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that students from deprived areas tend to have lower average module scores compared with their counterparts from more affluent areas. This observation underscores the potential need for targeted interventions aimed at supporting students facing socio-economic challenges. Breaking this down by module, we can see that while average scores vary, a gap remains across the modules.\nTo further understand the dynamics of this gap, we may want to look at how it has changed over time. Line charts, as in Figure 2, can serve as an effective visual tool, illustrating the trend of average scores by deprivation status across different time periods. In Figure 2 we can see there is a consistent gap over time across the courses, suggesting that the gap we have identified is persistent.\n\n\n\nFigure 2: Deprivation gaps over time\n\n\n\n\n\n\n\n(a) Over time, there are persistent gaps in average module scores between deprived students and their non-deprived peers\n\n\n\n\n\n\n\n\n\n\n\n(b) Between courses, there is variation in the size of the gap in average module scores between deprived students and their non-deprived peers\n\n\n\n\n\n\n\n\n\n\n\n\n\nT-tests\nIn Figure 1 above, we saw there was a 7.3 percentage point (pp) difference in scores between students from deprived areas (score = 48.2%) and students who are not from deprived areas (55.5%). However, looking at these charts alone, we cannot say whether the differences are meaningful or just random variation.\nWe can use t-tests to determine if the difference in mean scores between students from deprived backgrounds and those who are not deprived is statistically significant using the conventional cut-off for the p-value of 0.05.\nThe results of the t-test in Table 3, indicate that there is a statistically significant difference at the 5% level because the p-value is less than 0.05. This suggests that on average, students from deprived areas score significantly lower than their counterparts.\nTo report the results of this t-test in text you would write: There was a significant difference between the scores for deprived students and the scores for students who are not deprived (t(10686.68) = 15.88, p&lt;0.001).\n\n\n\n\nTable 3: T-test results\n\n\n\n\n\n\nDifference (pp)\nLower confidence interval\nUpper confidence interval\nT statistic\nDegrees of freedom\nP value\n\n\n\n\n7.35\n6.44\n8.25\n15.88\n10686.68\n&lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA\nBuilding on the concept of t-tests, we can extend our analysis to compare factors with more than two groups (e.g. ethnicity, prior qualification types). This is where ANOVA becomes useful. ANOVA allows us to assess whether there are statistically significant differences in means across three or more groups simultaneously.\nFor example, ANOVA can enable us to determine whether scores differ based on a student’s highest level of education. By performing ANOVA, we can compare the mean scores across each of the education levels at once, rather than comparing each pair separately. This approach helps to identify if at least one education group has a significantly different mean score compared to the others.\nIn Table 4, we can see that the ANOVA is significant (p&lt;0.001), indicating that there is evidence that the level of education influences the score.\n\n\n\n\nTable 4: ANOVA results\n\n\n\n\n\n\nTerm\nSum of squares\nDegrees of freedom\nMean square\nF statistic\nP value\n\n\n\n\nhighest_education\n362113.3\n4\n90528.3\n109.7\n&lt; 0.001\n\n\nResiduals\n15433168.8\n18703\n825.2\n\n\n\n\n\n\n\n\n\n\n\n\nGiven that there is evidence of a relationship between highest qualification and the scores, you can then conduct further tests such as Tukey’s HSD (a form of pairwise t-test) to determine which specific groups differ from each other, as shown in Table 5. This enables us to see, for example, that there is a significant difference in score between students with lower than A Level or equivalent and students with A Levels (highlighted pink), but not a significant difference between students with lower than A Level and students with no formal qualifications (highlighted green).\n\n\n\n\nTable 5: Tukey’s HSD results for module scores for all different pairwise comparisons of educational level\n\n\n\n\n\n\nComparison\nDifference\nLower confidence interval\nUpper confidence interval\nAdjusted p value\n\n\n\n\nHigher education qualification - A Level or equivalent\n2.57\n0.91\n4.23\n&lt;0.001\n\n\nLower than A Level - A Level or equivalent\n-7.88\n-9.15\n-6.60\n&lt;0.001\n\n\nNo formal qualifications - A Level or equivalent\n-14.70\n-21.60\n-7.80\n&lt;0.001\n\n\nPost-graduate qualification - A Level or equivalent\n9.03\n1.85\n16.20\n0.005\n\n\nLower than A Level - Higher education qualification\n-10.45\n-12.16\n-8.73\n&lt;0.001\n\n\nNo formal qualifications - Higher education qualification\n-17.27\n-24.26\n-10.28\n&lt;0.001\n\n\nPost-graduate qualification - Higher education qualification\n6.46\n-0.81\n13.72\n0.109\n\n\nNo formal qualifications - Lower than A Level\n-6.82\n-13.74\n0.09\n0.055\n\n\nPost-graduate qualification - Lower than A Level\n16.90\n9.72\n24.09\n&lt;0.001\n\n\nPost-graduate qualification - No formal qualifications\n23.73\n13.85\n33.61\n&lt;0.001\n\n\n\n\n\n\n\n\n\n\nFigure 3 visualises these differences. We can see both the difference in scores by highest education using descriptive measures (in this case the median score using a boxplot), and the statistically significant differences in score between these qualification levels.\n\n\n\nFigure 3: Gaps in score by highest education\n\n\n\n\n\n\n\n(a) Students with higher levels of educuation score higher on average\n\n\n\n\n\n\n\n\n\n\n\n(b) These differences are mostly statistically significant\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression\nDescriptive statistics, such as means or medians, and t-tests and ANOVAs are invaluable for highlighting disparities. But to explore in more depth, it can be useful to progress to regression analysis.\nRegression analysis helps identify the relationship between an outcome (like module score) and a factor (such as gender), while accounting for other demographic variables (such as ethnicity or socio-economic status). It is particularly useful for exploring how an outcome is influenced by the interaction of two or more factors. For example, regression analysis can show how the interaction of deprivation status and gender impacts modules scores, while also accounting for other factors that may affect scores.\nWhile descriptive statistics show us the extent of equality gaps, and t-tests and ANOVAs can tell us whether these gaps are statistically significant, regression analysis can give us some of the more detailed insights needed to provide targeted interventions.\n\nSimple model\nIn a basic linear regression model, we use only one explanatory variable - here deprivation status - to predict the dependent variable, which is the score.\nWhile this simple regression is comparable to our t-test in that it assesses the difference between two groups, it also allows us to quantify the exact impact of deprivation status on scores. Unlike the t-test, which only tells us if there is a significant difference, regression provides an estimate of how much lower deprived students score on average. Here, the regression results in Table 6 tell us that deprived students score, on average, 7 marks lower (see highlighted row).\n\n\n\n\nTable 6: Simple regression results\n\n\n\n\n\n\n\nSimple model\n\n\n\n\n(Intercept)\n55.537***\n\n\n\n(0.253)\n\n\ndeprivation_statusDeprived\n−7.347***\n\n\n\n(0.458)\n\n\nNum.Obs.\n18708\n\n\nR2\n0.014\n\n\nR2 Adj.\n0.014\n\n\nRMSE\n28.86\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\nWe can visualise this simple regression in a chart, as in Figure 4, with lines indicating our 95% confidence interval. However, this alone tells us relatively little about what impacts scores. Our adjusted R-squared of 0.014 tells us that deprivation status can only explain 1.4% of the variance in score.\n\n\n\n\n\nFigure 4: Deprived students score, on average, 7 marks lower\n\n\n\n\n\n\n\n\n\n\nSimple model with interaction term\nTo try and gain more detailed insights into how intersecting characteristics influence outcomes, we can introduce an interaction term. In Table 7, we add to our simple regression an interaction between deprivation and gender. By doing so, we can explore whether the impact of deprivation on students’ scores is moderated by their gender.\nUsing an interaction term like this allows us to identify patterns that may not be evident when looking at these factors independently. This can be important when designing interventions that are sensitive to the needs of different student groups, enabling a more targeted approach.\nWe see in Table 7 that while deprivation remains a significant predictor, the interaction between deprivation and gender is not significant. This indicates that the relationship between deprivation and score does not significantly differ across genders. In other words, the impact of being from a deprived area on the score is roughly the same for male students as it is for female students.\n\n\n\n\nTable 7: Simple regression with interaction results\n\n\n\n\n\n\n\nSimple model\n Interaction model\n\n\n\n\n(Intercept)\n55.537***\n56.176***\n\n\n\n(0.253)\n(0.387)\n\n\ndeprivation_statusDeprived\n−7.347***\n−7.860***\n\n\n\n(0.458)\n(0.662)\n\n\ngenderMale\n\n−1.116*\n\n\n\n\n(0.512)\n\n\ndeprivation_statusDeprived × genderMale\n\n0.863\n\n\n\n\n(0.919)\n\n\nNum.Obs.\n18708\n18708\n\n\nR2\n0.014\n0.014\n\n\nR2 Adj.\n0.014\n0.014\n\n\nRMSE\n28.86\n28.86\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFull model (multiple regression)\nWe can expand our model to include additional regressors such as module presentation, previous qualifications, age category, and virtual learning environment (VLE) clicks. Collectively, these factors should give a better view of the variables affecting student scores. By incorporating more predictors, we can identify more subtle relationships, and the impact of each factor on outcomes.\nIn Table 8, we see significant effects across many of our variables.\n\n\n\n\nTable 8: Full regression model results\n\n\n\n\n\n\n\nSimple model\n Interaction model\nFull model\n\n\n\n\n(Intercept)\n55.537***\n56.176***\n46.997***\n\n\n\n(0.253)\n(0.387)\n(0.651)\n\n\ndeprivation_statusDeprived\n−7.347***\n−7.860***\n−5.391***\n\n\n\n(0.458)\n(0.662)\n(0.402)\n\n\ngenderMale\n\n−1.116*\n−4.943***\n\n\n\n\n(0.512)\n(0.375)\n\n\ndeprivation_statusDeprived × genderMale\n\n0.863\n\n\n\n\n\n(0.919)\n\n\n\ndisabilityDisabled\n\n\n−5.676***\n\n\n\n\n\n(0.659)\n\n\ncode_presentation2013J\n\n\n3.069***\n\n\n\n\n\n(0.564)\n\n\ncode_presentation2014B\n\n\n−0.615\n\n\n\n\n\n(0.597)\n\n\ncode_presentation2014J\n\n\n3.824***\n\n\n\n\n\n(0.543)\n\n\nqualificationsHE or post-graduate qualification\n\n\n0.796\n\n\n\n\n\n(0.532)\n\n\nqualificationsLower than A Level or no formal qualifications\n\n\n−7.507***\n\n\n\n\n\n(0.407)\n\n\nnum_of_prev_attempts\n\n\n−4.128***\n\n\n\n\n\n(0.436)\n\n\nage_categoryUnder 35\n\n\n0.748+\n\n\n\n\n\n(0.412)\n\n\ntotal_vle_clicks_thousands\n\n\n7.171***\n\n\n\n\n\n(0.101)\n\n\nNum.Obs.\n18708\n18708\n18692\n\n\nR2\n0.014\n0.014\n0.255\n\n\nR2 Adj.\n0.014\n0.014\n0.255\n\n\nRMSE\n28.86\n28.86\n25.07\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we sort and plot these results, as in Figure 5, we get an indication of where it might be best to target an intervention. In particular, we can see that having the highest known qualification being below A Level, having a disability, and being from a deprived area have some of the biggest effects on score. This may prompt us to think about how we can devise interventions to target these groups.\nThis expanded approach allows us to improve our understanding of the factors that influence our outcome of interest, enabling us to prioritise intervention efforts and design interventions with these factors in mind.\n\n\n\n\n\nFigure 5: Being disabled, being deprived, and having lower qualification levels all have significant effects on average scores",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Diagnose"
    ]
  },
  {
    "objectID": "dig/diagnose.html#design-your-intervention",
    "href": "dig/diagnose.html#design-your-intervention",
    "title": "Diagnose: Identifying equality gaps using institutional data",
    "section": "Design your intervention",
    "text": "Design your intervention\n\n\nEvaluatorsPractitionersSenior management\n\n\n\nTheory of change\nOnce a gap has been diagnosed, you can start thinking about how an intervention could address this gap, and about evaluating this intervention.\nDeveloping a theory of change is the first step in TASO’s Monitoring and Evaluation Framework and the foundation of all evaluations. The theory of change should capture the reasons why you think the activities you are doing will cause the change you want to realise.\nTASO has a two-strand approach to theory of change development:\n\nStrand one – Core Theory of Change: used for simplicity and to assist higher education providers with planning interventions and evaluation activities. The Core Theory of Change guidance follows a simple model of mapping inputs, activities, outputs, outcomes and impact. It provides a high-level snapshot of how we expect an activity to lead to impact.\nStrand two – Enhanced Theory of Change: used for evaluability and to assist higher education providers with robustly evaluating interventions and activities. The Enhanced Theory of Change guidance provides a format for capturing much more information about activities and mechanisms by which we expect change to happen. It includes: context; mapping of links between activities and outcomes; and assumptions and change mechanisms.\n\n\n\n\n\n\n\nMore theory of change resources\n\n\n\nTASO has a range of resources for developing Core and Enhanced Theories of Change, including:\n\n\n\nCore Theory of Change templates\nTheory of Change workshop resources\nEnhanced Theory of Change templates\n\n\n\nTheories of Change for the ethnicity degree awarding gap (EDAG)\nTheories of Change for attainment-raising initiatives\n\n\n\n\n\n\n\nPost-entry typology\nTo effectively evaluate interventions aimed at improving student success, it’s important to record how students engage with those interventions. Additionally, developing a common language to describe the students involved, the outcomes achieved, and the activities undertaken would help improve our collective understanding across the sector of what works to support student success.\nThe post-entry MOAT (Mapping Outcomes and Activities Tool) provides a framework to describe your post-entry student support activities, in terms of their intended beneficiaries, outcomes (and suggested measures), their type and sub-type, how they are delivered and, if applicable, how they map on to the Equality of Opportunity Risk Register.\n\n\n\nFigure 6: Overview of the main features of the post-entry MOAT\n\n\n\n\n\n\nThe post-entry MOAT covers any HEP’s activity whose ultimate beneficiary is deemed to be its own students. This includes:\n\ntransition activities designed for students who have confirmed the HEP as a firm choice on their UCAS application\nactivities where individual students directly benefited and participated\nactivities in which staff or entire departments are the focus; e.g. activities focused on changing the curriculum or academic culture within a school, which may include staff training and administrative or communications activities\nactivities involving changes to physical or virtual infrastructure, e.g. improvements to lab or study facilities, or changes to the virtual learning environment.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Diagnose"
    ]
  },
  {
    "objectID": "dig/measure.html",
    "href": "dig/measure.html",
    "title": "Measure: Collection and analysis of data",
    "section": "",
    "text": "MeasureEvaluators\nOnce you have diagnosed an equality gap, planned your research questions, outcomes, and chosen your methods, you can start to measure your impact.\nBelow are some TASO examples of different QED and RCT designs that serve as useful case studies. These examples can help inform your own analysis, offering insights into how different approaches can be applied to measure impact effectively and accurately.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Measure"
    ]
  },
  {
    "objectID": "dig/measure.html#qeds",
    "href": "dig/measure.html#qeds",
    "title": "Measure: Collection and analysis of data",
    "section": "QEDs",
    "text": "QEDs\n\nMatching\n\n\n\n\n\n\nNTU: Black Leadership Programme (BLP) impact evaluation using propensity score matching\n\n\n\n\n\n\n  Download BLP impact evaluation  \n\nBackground: Staffordshire University was commissioned by the Centre for Transforming Access and Student Outcomes in Higher Education (TASO) to act as an independent evaluator of four post-entry interventions to address inequalities in student outcomes using institutional data and quasi-experimental designs. This report corresponds to the evaluation conducted for Nottingham Trent University’s Black Leadership Programme (BLP).\nAims: To explore whether the BLP impacts students’ social and academic engagement, and whether there is a relationship with degree outcomes and BLP participation that is mediated by academic engagement.\nIntervention: BLP is an intervention delivered during level 5 (2nd year undergraduate) for Black and Black heritage students that provides mentoring, social events, and a programme of workshops and development activities to support students’ self-concept, social capital and skills, such that they begin to engage more at NTU and ultimately progress to succeeding in higher education and in their lives outside higher education.\nDesign: This evaluation was a QED using available institutional data. A comparator group was developed with Propensity Score Matching using POLAR4 Quintile, UCAS entry points, Academic School, level of study, and academic year as matching variables.\nOutcome measures: Three primary outcome measures were included in the analyses:\n\nAcademic engagement – this was an amalgamated dataset of different types of academic engagement, including both structured and unstructured types of engagement.\nStructured social engagement – sports clubs and societies signed up to with the Students’ Union.\nUnstructured social engagement – whether students had signed up for a gym membership.\n\nThere was one secondary outcome measure:\n\nLevel 6 grade – this was used in place of final degree classification.\n\nAnalyses: A combination of logistic regression, ANOVA, and structural equation modelling were used to address the research questions.\nResults: Results suggest limited effect of BLP on social and academic engagement across the academic journey. However, BLP students were found to have higher level 6 grades, which may be explained by factors other than academic engagement.\nConclusions: BLP may have an impact on students that begins later than the year of the BLP programme, and is not through a direct relationship on students’ academic engagement.\n\n\n\n\n\n\n\n\n\nUEA: Peer Assisted Learning (PAL) impact evaluation using propensity score matching\n\n\n\n\n\n\n  Download PAL impact evaluation  \n\nBackground: Staffordshire University was commissioned by the Centre for Transforming Access and Student Outcomes in Higher Education (TASO) to act as an independent evaluator of four post-entry interventions to address inequalities in student outcomes using institutional data and quasi-experimental designs. This analysis report is the impact evaluation of the Peer Assisted Learning (PAL) programme at the University of East Anglia (UEA).\nAims: The aim of this study is to explore whether participation in PAL increases student engagement, continuation rates and attainment\nIntervention:\nPAL is open to all first-year students in participating schools of study on an opt-in basis. PAL consists of regularly scheduled mentoring sessions throughout the academic year. Prior to the academic year, schools and courses decide whether their course will deliver one-to-one peer mentoring or group mentoring. Group mentoring is formalised through the timetable and one-to-one mentoring is typically scheduled every three weeks.\nDesign: This study will use a post-hoc QED to determine the relationship between PAL participation and the outcome measures of interest.\nOutcome measures: There are three primary outcome measures for this study: course engagement, continuation to the next level of academic study and end of stage grades. In addition, there are two secondary outcome measures for this study: course completion and final degree classification.\nAnalyses: A matched control group was generated using propensity score matching to test the effect of PAL on student engagement and outcomes using ordinary least squares (OLS) or binary logistic regression (BLR) where appropriate.\nResults: Results suggest that participation in PAL significantly improves the likelihood of continuation after the first year of study; is significantly associated with higher course engagement and provides significant positive benefits to end of stage grades in the first year. There was no observable effect of PAL participation on final degree classification. There is evidence that some underrepresented student groups in higher education who participate in PAL have different continuation and end of stage grade outcomes than their peers.\nConclusions: This study provides evidence that participating in PAL supports first year student outcomes and equality of opportunity aims at UEA. The findings, while significant, produced small effects for the models tested. Further research is needed to build on these results including and understanding of how delivery mode and additional variables not captured in the models may impact the effectiveness of PAL for student mentees.\n\n\n\n\n\nDifference-in-difference\n\n\n\n\n\n\nUniversity of Leicester: Curriculum reform impact evaluation using differences in differences\n\n\n\n\n\n\n  Download curriculum reform impact evaluation  \n\nBackground: The Centre for Transforming Access and Student Outcomes in Higher Education (henceforth TASO) has funded the University of Leicester (henceforth Leicester) to develop and implement a “Decolonising the Curriculum Toolkit” (a resource for staff that provides concise guidelines on how to make their curriculum more racially inclusive). TASO has also commissioned the Behavioural Insights Team (henceforth BIT) to evaluate the impact of the toolkit on reducing awarding gaps between Black, Asian and Minority Ethnic (BAME) students and White students.\nAims: To evaluate how Leicester’s ‘Decolonising the Curriculum Toolkit’ affected the attainment of BAME and White students as well as the racial awarding gap.\nIntervention: The “Decolonising the Curriculum Toolkit” is a two-page resource for staff that provides clear and concise guidelines on how to make module content, assessment and teaching practice more racially inclusive and relatable for all students. The toolkit was piloted across the Sociology BA course in the 2020/21 academic year.\nDesign: This is a matched difference-in-differences study with repeated cross-sections. The analysis compares students’ attainment trends in the modules that implemented the “Decolonising the Curriculum Toolkit” (treatment modules) with that of similarly comparable modules that did not implement the initiative.\nOutcome measures: The primary outcome measure is a student’s module-level attainment, and it is defined as the percentile rank of the final module mark.\nAnalyses: The primary analysis consists of a difference-in-differences regression, comparing module marks before and after the academic year 2019-20 (the year that curriculum reform took place) between reformed vs. matched unreformed modules. It focuses on BAME students only. The secondary analysis repeats the primary analysis for White students. Additional descriptive line charts have been made to illustrate how the awarding gaps of reformed vs. comparator modules changed since the “Decolonising the Curriculum Toolkit” was implemented.\nResults: Overall, this impact evaluation suggests that the “Decolonising the Curriculum Toolkit’’ might have had a negative impact on both BAME and White students’ attainment among Sociology students at the University of Leicester. The estimated treatment effect was significantly negative among BAME students, -6.63 percentiles, 95% CI [-13.23, 0.03], p = 0.05. It was directionally negative (though not significant at the 5% level) among White students, -3.07 percentiles, 95% CI [-9.79, 3.64], p = 0.37. Findings from the exploratory analysis suggest that intervention did not affect the racial awarding gap.\nConclusions: In light of the above discussion, we do not recommend rolling out this toolkit (in its current form) before conducting a closer examination of how the toolkit was delivered by teachers and received by students. We believe the implementation and process evaluation (IPE) led by Leicester may shed light on what might have caused this and help contextualise these effects. If findings from the IPE suggest that there is evidence of promise in how teaching staff and BAME students might benefit from the reforms, we would recommend refining the toolkit based on the IPE and then conducting further impact evaluation of the intervention, with a larger sample and over a longer time period.\n\n\n\n\n\n\n\n\n\nUniversity of Kent: Curriculum reform impact evaluation using differences in differences\n\n\n\n\n\n\n  Download curriculum reform impact evaluation  \n\nBackground: The Centre for Transforming Access and Student Outcomes in Higher Education (henceforth TASO) has funded the University of Kent (henceforth Kent) and commissioned the Behavioural Insights Team (henceforth BIT) to evaluate the impact of their “Diversity Mark” programme (an initiative that seeks to diversify the current Eurocentric curriculum) on reducing awarding gaps between Black, Asian and minority ethnic (BAME) students and White students.\nAims: To evaluate whether and to which extent Kent’s ‘Diversity Mark’ initiative reduced the awarding gaps between BAME and White students.\nIntervention: The “Diversity Mark” initiative is a collaborative response to Kent students’ call for more diverse curricula. The School of Sociology, Social Policy and Social Research (henceforth SSPSSR), students, and library services worked together to audit 19 core undergraduate modules offered in the two campuses and explored ways to incorporate BAME authors and perspectives into those modules. The initiative was first piloted in 2018-19, and another module was piloted in 2020-21.\nDesign: The study is a matched difference-in-differences with repeated cross-sections. The analysis compares students’ attainment trend among the modules that implemented the Diversity Mark Initiative (treatment modules) with similar comparator modules that didn’t implement the initiative.\nOutcome measures: The primary outcome measure is a student’s module-level average attainment, and it is defined as the percentile rank of the final module mark.\nAnalyses: The primary analysis consists of a difference-in-differences regression, comparing module marks before and after the academic year 2018-19 between reformed vs. matched unreformed modules. It focuses on BAME students only. The secondary analysis repeats the primary analysis for White students. Additional descriptive charts are made to illustrate the change in awarding gaps of reformed vs. comparator modules before and after the Diversity Mark Initiative.\nResults: Among the modules matched for analyses (4 reformed modules, 4 comparator modules), we did not observe a significant effect of the Diversity Mark Initiative on improving attainment in terms of module mark percentile rank among the BAME students — the average difference in attainment between reformed and unreformed modules post-intervention versus pre-intervention was not statistically significant (2.0 percentile rank, 95% CI [-2.20, 6.21]), p = 0.35. We observed a marginally positive difference among the White students (3.45 percentile rank, 95% CI [-0.13, 7.03]), p = 0.06.\nConclusions: Overall, we didn’t find conclusive evidence supporting the effectiveness of the Diversity Mark Initiative in reducing the racial awarding gap among the SSPSSR students at the University of Kent. However, we also didn’t find evidence that suggests it might backfire: the observed trend among BAME students’ attainment before and after the initiative, though not significant, was positive. Therefore, we consider the initiative an innovative approach to address the racial awarding gap that is worth further testing.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Measure"
    ]
  },
  {
    "objectID": "dig/measure.html#rcts",
    "href": "dig/measure.html#rcts",
    "title": "Measure: Collection and analysis of data",
    "section": "RCTs",
    "text": "RCTs\n\n\n\n\n\n\nNottingham Trent University: Learner Analytics Randomised Controlled Trial (RCT)\n\n\n\n\n\n\n  Download learning analytics impact evaluation  \n\nBackground: The Behavioural Insights Team (BIT) was commissioned by the Centre for Transforming Access and Student Outcomes in Higher Education (TASO) to act as an independent evaluator of two randomised controlled trials. Both trials were designed to assess the impact of learning analytics interventions. This report corresponds to the trial delivered at Nottingham Trent University (NTU).\nAims: To evaluate whether a preventative intervention targeted at students that generate a no-engagement alert via NTU’s learning analytics student dashboard (StREAM) increased student engagement.\nIntervention:\n\nIn the intervention 1 group, students who generated a no-engagement alert received up to two phone call attempts from NTU’s central support team (business as usual).\nIn the intervention 2 group, students who generated a no-engagement alert received an email inviting them to request a phone call.\n\nDesign: This study was a two-arm, parallel group randomised controlled trial, testing for superiority of the intervention 1 condition over the intervention 2 condition.\nOutcome measures: There were two primary outcomes:\n\nAverage daily student engagement rating in the 10 day period following their first no-engagement flag (days 1 to 10 of the intervention period) and II.\nAverage daily student engagement rating in the first four-week period of Term 2.\n\nThese outcomes were collected by NTU’s learning analytics system, which involves daily reporting on individual-level engagement data.\nAnalyses: A combination of logistic and ordinary least squares (OLS) regressions was used, as appropriate, to estimate effects on the primary and secondary outcomes.\nResults: The primary analysis suggests no benefit to students of intervention 1 (automatic phone call) over intervention 2 (email). Estimated effects on the primary outcomes and first secondary outcome are either null or narrowly negative, and none are statistically significant at the 5% level. The impact table for the results is in Appendix C.\n\n\n\n\n\n\n\n\n\nSheffield Hallam University: Learner Analytics Randomised Controlled Trial (RCT)\n\n\n\n\n\n\n  Download learning analytics impact evaluation  \n\nBackground: The Behavioural Insights Team (BIT) was commissioned by the Centre for Transforming Access and Student Outcomes in Higher Education (TASO) to act as an independent evaluator of two randomised controlled trials. Both trials were designed to assess the impact of learning analytics interventions. This report corresponds to the trial delivered at Sheffield Hallam University (SHU).\nAims: To evaluate whether a preventative intervention targeted at students identified as being ‘at-risk’ via SHU’s learning analytics programme increases student engagement.\nIntervention: Student Support Advisers (SSAs) from a central team proactively monitored engagement at two pre-agreed census points (week 5 and 8 of the autumn term) to identify students who have poor engagement with their course.\n\nIn the intervention 1 group students who generated a red flag (indicating low engagement) in week 4 and/or week 7 received an email detailing support resources available to them plus a text message (SMS) informing them that they will receive a default phone call from a central support team. An SSA then attempted to call all students.\nIn the intervention 2 group students who generated a red flag (indicating low engagement) in week 4 and/or week 7 received an email detailing support resources available to them. No telephone calls were made.\n\nDesign: This study was a two-arm, parallel group randomised controlled trial, testing for superiority of the intervention 1 condition over the intervention 2 condition.\nOutcome measures: There was one primary outcome, the proportion of Red RAG engagement scores at week 9 (defined in section 3.3).\nAnalyses: A combination of logistic and ordinary least squares (OLS) regressions was used, as appropriate, to estimate effects on the primary and secondary outcomes.\nResults: The primary analysis suggests no benefit to students of intervention 1 over intervention 2. All estimated effects are small, and none are statistically significant at the 5% level.\n\n\n\n\n\n\n\n\n\nSummer Schools Randomised Controlled Trial (RCT)\n\n\n\n\n\n\n  Download summer schools impact evaluation  \n\nBackground: This project is a collaboration between the Centre for Transforming Access and Student Outcomes in Higher Education (TASO), five Higher Education Providers (HEPs) and the Behavioural Insights Team (BIT). In summer 2022, a series of summer schools were delivered with the aim of widening participation in higher education (HE) among participants. Three types of evaluation are being conducted with these summer schools: an impact evaluation, a cost evaluation, and an implementation and process evaluation (IPE). This report presents the interim findings from the impact evaluation.\nAims: The aim of the project is to investigate the efficacy of summer schools as a widening participation activity. The aim of the widening participation agenda is to increase progression to HE among students from disadvantaged or under-represented groups.\nIntervention: This study evaluated a collection of interventions. Five HEPs delivered their own summer schools, either for students in pre-16 or post-16 education. Mode of delivery differed, with some summer schools taking place in-person, and some using a combination of online and in-person elements.\nDesign: This study is a two-arm, parallel group randomised controlled trial (RCT).\nOutcome measures: The outcomes analysed in this interim report are survey measures of participants’ self-reported applications to HE, and self-reported attitudes to HE, covering their likelihood of going on to further academic study (for pre-16 students), their self-efficacy relating to HE, the compatibility of HE with their social identity, and their perception of practical barriers to HE.\nAnalyses: A combination of logistic and Ordinary Least Squares (OLS) regressions are used, as appropriate, to estimate effects on the primary, secondary and exploratory outcomes.\nResults: There is a positive effect on students’ sense of the compatibility of HE with their identity, which is significant at the 5% or 10% level depending on the model specification. Results are not highly consistent across model specifications. None of the other effects are statistically significant at the 5% or 10% level. A high and differential rate of attrition has led to a small sample and possible bias in some of the estimated effects.\nConclusions: There is early evidence of promise that these summer schools had a small positive effect on the hypothesised mediating mechanism of compatibility of HE with social identity. The analysis also suggests that there was no effect on self-reported applications to HE, self-efficacy relating to HE, students’ self-reported likelihood of attending HE or post-16 academic study (depending on their age), or perception of practical barriers to HE. This is probably because most applicants to HE summer schools already intend to apply to HE. The more robust test of the intervention will come in 2025 when we have administrative data on students’ entry to HE.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Measure"
    ]
  },
  {
    "objectID": "dig/plan-methods.html",
    "href": "dig/plan-methods.html",
    "title": "Plan: Choosing methods",
    "section": "",
    "text": "PlanEvaluatorsPractitionersSenior management",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Choosing methods"
    ]
  },
  {
    "objectID": "dig/plan-methods.html#sec-choosing-an-evaluation-design",
    "href": "dig/plan-methods.html#sec-choosing-an-evaluation-design",
    "title": "Plan: Choosing methods",
    "section": "Choosing an evaluation design",
    "text": "Choosing an evaluation design\nWhen choosing an evaluation design, there are many factors that will influence your decision, including the nature of the intervention, the availability of data, and any ethical considerations.\nProduction of type 3 (causal) evidence will often entail using experimental (i.e. a randomised controlled trial, RCT) or quasi-experimental (e.g. difference-in-difference, or regression discontinuity design) methods. The questions in Figure 1 will help you consider which experimental or quasi-experimental methods would be most appropriate and feasible for your evaluation.\n\n  Download flowchart as PDF  \n\n\n\n\nFigure 1: Flowchart for choosing between experimental and quasi-experimental methods",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Choosing methods"
    ]
  },
  {
    "objectID": "dig/plan-methods.html#planning-the-analysis",
    "href": "dig/plan-methods.html#planning-the-analysis",
    "title": "Plan: Choosing methods",
    "section": "Planning the analysis",
    "text": "Planning the analysis\n\nResourcing checklist\n\nData\n\n\n Do you know what data you need to collect?\n Do you know where the data you need is located?\n Is the data spread across multiple systems?\n\n Is the data accessible?\n Is the data available for the timeframe you need it?\n\n\n\n\n\nPeople\n\n\n Do you know who is going to collect the data, either by obtaining it from the data repository or, for surveys and interviews, obtaining it directly from participants?\n\n Do you know who is going to analyse and interpret the data?\n Do you know who is going to write up the analysis?\n\n\n\n\n\nSystems\n\n\n Are the required privacy notices in place?\n\n Do you have or need ethical approval to gather the data or conduct the analysis? Note: if you are publishing a report (and we strongly encourage you to do so) then you will require some form of ethical approval to conduct the evaluation.\n\n\n\n\n\nWriting an analysis plan/trial protocol\nA research protocol (also known as an analysis plan or trial protocol) is a written document that describes the overall approach that will be used throughout your intervention, including its evaluation.\nA research protocol is important because it:\n\nlays out a cohesive approach to your planning, implementation and evaluation\ndocuments your processes and helps create a shared understanding of aims and results\nhelps anticipate and mitigate potential challenges\nforms a basis for the management of the project and the assessment of its overall success\ndocuments the practicalities of implementation.\n\nReasons for creating a research protocol include:\n\nSetting out what you are going to do in advance is an opportunity to flush out any challenges and barriers before going into the field.\nWriting a detailed protocol allows others to replicate your intervention and evaluation methodology, which is an important aspect of contributing to the broader research community.\nSetting out your rationale and expectations for the research, and your analysis plan, before doing the research gives your results additional credibility.\n\nThe protocol should be written as if it is going to end up in the hands of someone who knows very little about your organisation, the reason for the research, or the intervention. This is to future-proof the protocol, but also to ensure that you document all your thinking and the decisions you have made along the way.\n\n\n\n\n\n\nTASO research protocol templates\n\n\n\nTASO has useful templates for researchers and evaluators writing protocols, including:\n\n\n\nTrial protocol template\nQualitative research protocol template\nRapid evidence review protocol template\n\n\n\nImplementation and process evaluation (IPE) protocol template\nEconomic evaluation protocol template\n\n\n\n\n\n\n\nEthical approval\nEthical approval is an important component of any research or evaluation where any of the following are true\n\nDifferent students take part in different versions of an activity due to randomisation or selection, and\nThe results of the evaluation will be made publicly available\n\nThe requirements of the APPs to plan, carry out and publish the results of evaluation mean that ethical approval should be sought.\nThis presents an opportunity for ethics review boards to set up a dedicated process for evaluation of APP-related activity, which, when using data that is routinely collected or involving the analysis of historical data, is likely to be lower risk than when collecting new data.\nA dedicated APP ethical approval process would likely need to consider the following:\n\nDistinguishing between evaluation that uses data that is routinely collected (i.e. institutional data) and evaluation that requires the collection of new data.\nWhat outcomes will be tested and the analysis methods used.\nSteps to plan for qualitative data collection (e.g. interviews or focus groups) for implementation and process evaluation (IPE).\n\n\n\n\n\n\n\nExample of a dedicated ethical approval process for APP evaluations\n\n\n\nDr Matt Horton at the University of Wolverhampton has set up a dedicated panel for ethical approval of APP work to encourage evaluation in this space.\nThe purposes of the panel are to improve responsiveness (applications are reviewed on a monthly basis), simplify the application process (reducing the form to questions only relevant to the APP) and to provide a standardised participant information and consent form.\nThe application form collects the following information\n\nthe theory of change for the intervention\nstaff involved in the evaluation\nthe timeframe of the evaluation\nan outline of the research questions\nthe type of evidence that will be obtained\nthe relevant stage in the student lifecycle (access, success, progression)\nhow data will be analysed and disseminated\nwhether relevant staff have or need Disclosure Barring Service (DBS) certificate (e.g., for pre-entry work).\nhow data will be kept secure and confidential\n\nThe development of a single APP ethics board makes it easier for the responsible manager to support staff in submitting approvals. Further, the form also collects information that can be used for the OfS evaluation self-assessment tool.\n\n\nTASO’s Research Ethics Guidance Document provides detailed advice on how to conduct research/evaluation safely and ethically, covering a range of common ethical issues.\n\n\n\n\n\n\nExcerpt from TASO’s Research Ethics Guidance\n\n\n\nIt is worth saying at the outset that ethical scrutiny is not intended to stop good research/evaluation, but it is intended to:\n\nprotect the human rights of participants in research/evaluation\nensure that risks are considered and mitigated and that harms do not disproportionately fall on one social group\nmaintain society’s confidence in the ethical self-regulation of the field\nprotect practitioners from claims of unethical behaviour.\n\nWe hope it is clear that ethics cannot be subordinated to researcher/evaluator convenience nor to the additional costs and resources that ethical research/evaluation might incur.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Plan: Choosing methods"
    ]
  },
  {
    "objectID": "dig/reflect.html",
    "href": "dig/reflect.html",
    "title": "Reflect: Reporting, interpretation and refinement of intervention",
    "section": "",
    "text": "ReflectEvaluatorsPractitionersSenior management",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Reflect"
    ]
  },
  {
    "objectID": "dig/reflect.html#reporting",
    "href": "dig/reflect.html#reporting",
    "title": "Reflect: Reporting, interpretation and refinement of intervention",
    "section": "Reporting",
    "text": "Reporting\nGenerating evidence can only get us so far. Ultimately, it doesn’t matter how great an educational idea or intervention is on paper; what really matters is how it manifests itself in the day-to-day work of students and educational stakeholders. It is therefore crucial that findings of all evaluations are shared to enable learning across an institution.\nInstitutions deploying widening participation activities are learning organisations. They continuously strive to do better for the participants and staff in their charge. In doing so, they try new things, seek to learn from those experiences, and work to adopt and embed the practices that work best. There has been growing recognition over the last 20 years that simply ‘packaging and posting’ research is unlikely, by itself, to impact significantly on decision-making and behaviours.\n\n\n\n\n\n\nTASO reporting templates\n\n\n\nTASO have useful reporting templates for researchers and evaluators, including:\n\n\n\nFinal project report template\nAnalysis report template\n\n\n\nImplementation and process evaluation (IPE) reporting template\nEconomic evaluation report template",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Reflect"
    ]
  },
  {
    "objectID": "dig/reflect.html#putting-evidence-to-work",
    "href": "dig/reflect.html#putting-evidence-to-work",
    "title": "Reflect: Reporting, interpretation and refinement of intervention",
    "section": "Putting evidence to work",
    "text": "Putting evidence to work\nWhen writing up your evidence report, your writing should be guided by your research protocol and should focus on answering the research questions identified. You should present expected and unexpected results as this will enable further learning and facilitate the adaptation of theories of change and the interventions themselves. It is worth considering on how to sustain the consistent and intelligent implementation of your findings in future iterations of the programme.\nDepending on the extent of changes that result from your findings, implementation of these can be – at the same time – tiring, energising, ambitious or overwhelming. It is important to be realistic about your institutional ‘implementation readiness’ and whether motivation, general capacity and programme-specific skills need to be developed. For example, the loss of key staff or advocates can crucially change how your evaluative findings (and their consequent implementation) can be perceived, while a reduction in budgets or staff resources can limit their use.\nTo avoid deadlocks, consider these possibilities at the early stages of an evaluation approach and use the reflective stage to revisit and consider any discrepancies between the expected and actual findings. The risks and assumptions section of your theory of change should be used to highlight contingency plans for potential turnover of staff, or to consider additional funding sources to maintain the innovation over time. To ensure that these kind of stresses do not affect the successful implementation of your evaluation and its consequent findings, it is recommended to take regular ‘pulse checks’ across your key stakeholders.\nOnce your evaluation findings lead to the implementation of your intervention as ‘business as usual’, it is important to continue monitoring and tracking that implementation in order to capture how the intervention, in its full roll-out, is behaving and whether your underlying assumptions, contexts and logical chains are still matching the actual implementation in its scaled-up format.",
    "crumbs": [
      "Home",
      "Data infrastructure guide",
      "Reflect"
    ]
  }
]